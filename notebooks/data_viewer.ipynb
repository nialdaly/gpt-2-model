{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/gpt-2_data_300.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>task</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>paper_abs</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The LovÌÁsz-Softmax loss: A tractable surrogat...</td>\n",
       "      <td>https://arxiv.org/pdf/1705.08790v2.pdf</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>Mean IoU</td>\n",
       "      <td>79.00%</td>\n",
       "      <td>Deeplab-v2 with Lovasz-Softmax loss</td>\n",
       "      <td>https://arxiv.org/abs/1705.08790v2.pdf</td>\n",
       "      <td>Abstract:  The Jaccard index, also referred to...</td>\n",
       "      <td>Accepted as a conference paper at CVPR 2018 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bottom-Up and Top-Down Attention for Image Cap...</td>\n",
       "      <td>https://arxiv.org/pdf/1707.07998v3.pdf</td>\n",
       "      <td>Visual Question Answering</td>\n",
       "      <td>Percentage correct</td>\n",
       "      <td>70.34</td>\n",
       "      <td>Up-Down</td>\n",
       "      <td>https://arxiv.org/abs/1707.07998v3.pdf</td>\n",
       "      <td>Abstract:  Top-down visual attention mechanism...</td>\n",
       "      <td>Bottom-Up and Top-Down Attention for Image Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Fried Convnets</td>\n",
       "      <td>https://arxiv.org/pdf/1412.7149v4.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>Percentage error</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Deep Fried Convnets</td>\n",
       "      <td>https://arxiv.org/abs/1412.7149v4.pdf</td>\n",
       "      <td>Abstract:  The fully connected layers of a dee...</td>\n",
       "      <td>Deep Fried Convnets Zichao Yang1 Marcin Moczu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Learning Natural Language Inference using Bidi...</td>\n",
       "      <td>https://arxiv.org/pdf/1605.09090v1.pdf</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>83.3</td>\n",
       "      <td>600D (300+300) BiLSTM encoders</td>\n",
       "      <td>https://arxiv.org/abs/1605.09090v1.pdf</td>\n",
       "      <td>Abstract:  In this paper, we proposed a senten...</td>\n",
       "      <td>() ar X iv :1 60 5. 09 09 0v 1 [ cs .C L ] 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cascade R-CNN: Delving into High Quality Objec...</td>\n",
       "      <td>https://arxiv.org/pdf/1712.00726v1.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>Bounding Box AP</td>\n",
       "      <td>42.8</td>\n",
       "      <td>Cascade R-CNN</td>\n",
       "      <td>https://arxiv.org/abs/1712.00726v1.pdf</td>\n",
       "      <td>Abstract:  In object detection, an intersectio...</td>\n",
       "      <td>ar X iv :1 71 2. 00 72 6v 1 [ cs .C V ] 3 D e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         paper_title  \\\n",
       "0  The LovÌÁsz-Softmax loss: A tractable surrogat...   \n",
       "1  Bottom-Up and Top-Down Attention for Image Cap...   \n",
       "2                                Deep Fried Convnets   \n",
       "3  Learning Natural Language Inference using Bidi...   \n",
       "4  Cascade R-CNN: Delving into High Quality Objec...   \n",
       "\n",
       "                                paper_url                        task  \\\n",
       "0  https://arxiv.org/pdf/1705.08790v2.pdf       Semantic Segmentation   \n",
       "1  https://arxiv.org/pdf/1707.07998v3.pdf   Visual Question Answering   \n",
       "2   https://arxiv.org/pdf/1412.7149v4.pdf        Image Classification   \n",
       "3  https://arxiv.org/pdf/1605.09090v1.pdf  Natural Language Inference   \n",
       "4  https://arxiv.org/pdf/1712.00726v1.pdf            Object Detection   \n",
       "\n",
       "          metric_name metric_value                                model  \\\n",
       "0            Mean IoU       79.00%  Deeplab-v2 with Lovasz-Softmax loss   \n",
       "1  Percentage correct        70.34                              Up-Down   \n",
       "2    Percentage error          0.7                  Deep Fried Convnets   \n",
       "3     % Test Accuracy         83.3       600D (300+300) BiLSTM encoders   \n",
       "4     Bounding Box AP         42.8                        Cascade R-CNN   \n",
       "\n",
       "                                paper_abs  \\\n",
       "0  https://arxiv.org/abs/1705.08790v2.pdf   \n",
       "1  https://arxiv.org/abs/1707.07998v3.pdf   \n",
       "2   https://arxiv.org/abs/1412.7149v4.pdf   \n",
       "3  https://arxiv.org/abs/1605.09090v1.pdf   \n",
       "4  https://arxiv.org/abs/1712.00726v1.pdf   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Abstract:  The Jaccard index, also referred to...   \n",
       "1  Abstract:  Top-down visual attention mechanism...   \n",
       "2  Abstract:  The fully connected layers of a dee...   \n",
       "3  Abstract:  In this paper, we proposed a senten...   \n",
       "4  Abstract:  In object detection, an intersectio...   \n",
       "\n",
       "                                          paper_text  \n",
       "0   Accepted as a conference paper at CVPR 2018 T...  \n",
       "1   Bottom-Up and Top-Down Attention for Image Ca...  \n",
       "2   Deep Fried Convnets Zichao Yang1 Marcin Moczu...  \n",
       "3   () ar X iv :1 60 5. 09 09 0v 1 [ cs .C L ] 3 ...  \n",
       "4   ar X iv :1 71 2. 00 72 6v 1 [ cs .C V ] 3 D e...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>task</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>paper_abs</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The LovÌÁsz-Softmax loss: A tractable surrogat...</td>\n",
       "      <td>https://arxiv.org/pdf/1705.08790v2.pdf</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>Mean IoU</td>\n",
       "      <td>79.00%</td>\n",
       "      <td>Deeplab-v2 with Lovasz-Softmax loss</td>\n",
       "      <td>https://arxiv.org/abs/1705.08790v2.pdf</td>\n",
       "      <td>Abstract:  The Jaccard index, also referred to...</td>\n",
       "      <td>Accepted as a conference paper at CVPR 2018 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bottom-Up and Top-Down Attention for Image Cap...</td>\n",
       "      <td>https://arxiv.org/pdf/1707.07998v3.pdf</td>\n",
       "      <td>Visual Question Answering</td>\n",
       "      <td>Percentage correct</td>\n",
       "      <td>70.34</td>\n",
       "      <td>Up-Down</td>\n",
       "      <td>https://arxiv.org/abs/1707.07998v3.pdf</td>\n",
       "      <td>Abstract:  Top-down visual attention mechanism...</td>\n",
       "      <td>Bottom-Up and Top-Down Attention for Image Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Fried Convnets</td>\n",
       "      <td>https://arxiv.org/pdf/1412.7149v4.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>Percentage error</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Deep Fried Convnets</td>\n",
       "      <td>https://arxiv.org/abs/1412.7149v4.pdf</td>\n",
       "      <td>Abstract:  The fully connected layers of a dee...</td>\n",
       "      <td>Deep Fried Convnets Zichao Yang1 Marcin Moczu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Learning Natural Language Inference using Bidi...</td>\n",
       "      <td>https://arxiv.org/pdf/1605.09090v1.pdf</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>83.3</td>\n",
       "      <td>600D (300+300) BiLSTM encoders</td>\n",
       "      <td>https://arxiv.org/abs/1605.09090v1.pdf</td>\n",
       "      <td>Abstract:  In this paper, we proposed a senten...</td>\n",
       "      <td>() ar X iv :1 60 5. 09 09 0v 1 [ cs .C L ] 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cascade R-CNN: Delving into High Quality Objec...</td>\n",
       "      <td>https://arxiv.org/pdf/1712.00726v1.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>Bounding Box AP</td>\n",
       "      <td>42.8</td>\n",
       "      <td>Cascade R-CNN</td>\n",
       "      <td>https://arxiv.org/abs/1712.00726v1.pdf</td>\n",
       "      <td>Abstract:  In object detection, an intersectio...</td>\n",
       "      <td>ar X iv :1 71 2. 00 72 6v 1 [ cs .C V ] 3 D e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fast and Accurate Deep Network Learning by Exp...</td>\n",
       "      <td>https://arxiv.org/pdf/1511.07289v5.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>Percentage correct</td>\n",
       "      <td>93.5</td>\n",
       "      <td>Exponential Linear Units</td>\n",
       "      <td>https://arxiv.org/abs/1511.07289v5.pdf</td>\n",
       "      <td>Abstract:  We introduce the \"exponential linea...</td>\n",
       "      <td>Published as a conference paper at ICLR 2016 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Language Modeling with Gated Convolutional Net...</td>\n",
       "      <td>https://arxiv.org/pdf/1612.08083v3.pdf</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>PPL</td>\n",
       "      <td>31.9</td>\n",
       "      <td>GCNN-14 bottleneck</td>\n",
       "      <td>https://arxiv.org/abs/1612.08083v3.pdf</td>\n",
       "      <td>Abstract:  The pre-dominant approach to langua...</td>\n",
       "      <td>Language Modeling with Gated Convolutional Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pervasive Attention: 2D Convolutional Neural N...</td>\n",
       "      <td>https://arxiv.org/pdf/1808.03867v3.pdf</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>BLEU score</td>\n",
       "      <td>27.99</td>\n",
       "      <td>Pervasive Attention</td>\n",
       "      <td>https://arxiv.org/abs/1808.03867v3.pdf</td>\n",
       "      <td>Abstract:  Current state-of-the-art machine tr...</td>\n",
       "      <td>Pervasive Attention: 2D Convolutional Neural ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Deep Bayesian Bandits Showdown: An Empirical C...</td>\n",
       "      <td>https://arxiv.org/pdf/1802.09127v1.pdf</td>\n",
       "      <td>Multi-Armed Bandits</td>\n",
       "      <td>Cumulative regret</td>\n",
       "      <td>1.92</td>\n",
       "      <td>NeuralLinear FullPosterior-MR</td>\n",
       "      <td>https://arxiv.org/abs/1802.09127v1.pdf</td>\n",
       "      <td>Abstract:  Recent advances in deep reinforceme...</td>\n",
       "      <td>Published as a conference paper at ICLR 2018 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CoupleNet: Coupling Global Structure with Loca...</td>\n",
       "      <td>https://arxiv.org/pdf/1708.02863v1.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>MAP</td>\n",
       "      <td>82.70%</td>\n",
       "      <td>CoupleNet</td>\n",
       "      <td>https://arxiv.org/abs/1708.02863v1.pdf</td>\n",
       "      <td>Abstract:  The region-based Convolutional Neur...</td>\n",
       "      <td>CoupleNet: Coupling Global Structure with Loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Semi-Supervised Classification with Graph Conv...</td>\n",
       "      <td>https://arxiv.org/pdf/1609.02907v4.pdf</td>\n",
       "      <td>Node Classification</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>70.30%</td>\n",
       "      <td>GCN</td>\n",
       "      <td>https://arxiv.org/abs/1609.02907v4.pdf</td>\n",
       "      <td>Abstract:  We present a scalable approach for ...</td>\n",
       "      <td>Published as a conference paper at ICLR 2017 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Best of Both Worlds: Combining Recent Adva...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.09849v2.pdf</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>BLEU score</td>\n",
       "      <td>41.0*</td>\n",
       "      <td>RNMT+</td>\n",
       "      <td>https://arxiv.org/abs/1804.09849v2.pdf</td>\n",
       "      <td>Abstract:  The past year has witnessed rapid a...</td>\n",
       "      <td>The Best of Both Worlds: Combining Recent Adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Brain Tumor Segmentation with Deep Neural Netw...</td>\n",
       "      <td>https://arxiv.org/pdf/1505.03540v3.pdf</td>\n",
       "      <td>Brain Tumor Segmentation</td>\n",
       "      <td>Dice Score</td>\n",
       "      <td>0.88</td>\n",
       "      <td>InputCascadeCNN</td>\n",
       "      <td>https://arxiv.org/abs/1505.03540v3.pdf</td>\n",
       "      <td>Abstract:  In this paper, we present a fully a...</td>\n",
       "      <td>Brain Tumor Segmentation with Deep Neural Net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Conditional Image Synthesis With Auxiliary Cla...</td>\n",
       "      <td>https://arxiv.org/pdf/1610.09585v4.pdf</td>\n",
       "      <td>Conditional Image Generation</td>\n",
       "      <td>Inception score</td>\n",
       "      <td>8.25</td>\n",
       "      <td>AC-GAN</td>\n",
       "      <td>https://arxiv.org/abs/1610.09585v4.pdf</td>\n",
       "      <td>Abstract:  Synthesizing high resolution photor...</td>\n",
       "      <td>Conditional Image Synthesis with Auxiliary Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Supervised Learning of Universal Sentence Repr...</td>\n",
       "      <td>https://arxiv.org/pdf/1705.02364v5.pdf</td>\n",
       "      <td>Semantic Textual Similarity</td>\n",
       "      <td>MRPC</td>\n",
       "      <td>76.2/83.1</td>\n",
       "      <td>InferSent</td>\n",
       "      <td>https://arxiv.org/abs/1705.02364v5.pdf</td>\n",
       "      <td>Abstract:  Many modern NLP systems rely on wor...</td>\n",
       "      <td>ar X iv :1 70 5. 02 36 4v 5 [ cs .C L ] 8 J u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Adversarial Training Methods for Semi-Supervis...</td>\n",
       "      <td>https://arxiv.org/pdf/1605.07725v3.pdf</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>94.1</td>\n",
       "      <td>Virtual adversarial training</td>\n",
       "      <td>https://arxiv.org/abs/1605.07725v3.pdf</td>\n",
       "      <td>Abstract:  Adversarial training provides a mea...</td>\n",
       "      <td>() ar X iv :1 60 5. 07 72 5v 3 [ st at .M L ]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>On the Role of Text Preprocessing in Neural Ne...</td>\n",
       "      <td>https://arxiv.org/pdf/1707.01780v3.pdf</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>88.9</td>\n",
       "      <td>CNN+LSTM</td>\n",
       "      <td>https://arxiv.org/abs/1707.01780v3.pdf</td>\n",
       "      <td>Abstract:  Text preprocessing is often the fir...</td>\n",
       "      <td>ar X iv :1 70 7. 01 78 0v 3 [ cs .C L ] 2 3 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Natural TTS Synthesis by Conditioning WaveNet ...</td>\n",
       "      <td>https://arxiv.org/pdf/1712.05884v2.pdf</td>\n",
       "      <td>Speech Synthesis</td>\n",
       "      <td>Mean Opinion Score</td>\n",
       "      <td>4.526</td>\n",
       "      <td>Tacotron 2</td>\n",
       "      <td>https://arxiv.org/abs/1712.05884v2.pdf</td>\n",
       "      <td>Abstract:  This paper describes Tacotron 2, a ...</td>\n",
       "      <td>NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Scale-Aware Trident Networks for Object Detection</td>\n",
       "      <td>https://arxiv.org/pdf/1901.01892v1.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>Bounding Box AP</td>\n",
       "      <td>48.4</td>\n",
       "      <td>TridentNet</td>\n",
       "      <td>https://arxiv.org/abs/1901.01892v1.pdf</td>\n",
       "      <td>Abstract:  Scale variation is one of the key c...</td>\n",
       "      <td>Scale-Aware Trident Networks for Object Detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Deeply learned face representations are sparse...</td>\n",
       "      <td>https://arxiv.org/pdf/1412.1265v1.pdf</td>\n",
       "      <td>Face Verification</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>99.47%</td>\n",
       "      <td>DeepId2+</td>\n",
       "      <td>https://arxiv.org/abs/1412.1265v1.pdf</td>\n",
       "      <td>Abstract:  This paper designs a high-performan...</td>\n",
       "      <td>Deeply learned face representations are spars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hyperspectral Image Classification with Markov...</td>\n",
       "      <td>https://arxiv.org/pdf/1705.00727v2.pdf</td>\n",
       "      <td>Hyperspectral Image Classification</td>\n",
       "      <td>Overall Accuracy</td>\n",
       "      <td>96.12%</td>\n",
       "      <td>CNN-MRF</td>\n",
       "      <td>https://arxiv.org/abs/1705.00727v2.pdf</td>\n",
       "      <td>Abstract:  This paper presents a new supervise...</td>\n",
       "      <td>1 Hyperspectral Image Classification with Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A Deep Relevance Matching Model for Ad-hoc Ret...</td>\n",
       "      <td>https://arxiv.org/pdf/1711.08611v1.pdf</td>\n",
       "      <td>Ad-Hoc Information Retrieval</td>\n",
       "      <td>MAP</td>\n",
       "      <td>0.279</td>\n",
       "      <td>DRMM</td>\n",
       "      <td>https://arxiv.org/abs/1711.08611v1.pdf</td>\n",
       "      <td>Abstract:  In recent years, deep neural networ...</td>\n",
       "      <td>ar X iv :1 71 1. 08 61 1v 1 [ cs .I R ] 2 3 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Towards a Better Match in Siamese Network Base...</td>\n",
       "      <td>https://arxiv.org/pdf/1809.01368v1.pdf</td>\n",
       "      <td>Visual Object Tracking</td>\n",
       "      <td>Expected Average Overlap (EAO)</td>\n",
       "      <td>0.337</td>\n",
       "      <td>SA Siam R</td>\n",
       "      <td>https://arxiv.org/abs/1809.01368v1.pdf</td>\n",
       "      <td>Abstract:  Recently, Siamese network based tra...</td>\n",
       "      <td>Towards a Better Match in Siamese Network Bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Learning Discriminative Features with Multiple...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.01438v3.pdf</td>\n",
       "      <td>Person Re-Identification</td>\n",
       "      <td>Rank-1</td>\n",
       "      <td>88.7</td>\n",
       "      <td>MGN</td>\n",
       "      <td>https://arxiv.org/abs/1804.01438v3.pdf</td>\n",
       "      <td>Abstract:  The combination of global and parti...</td>\n",
       "      <td>Learning Discriminative Features with Multipl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>How far are we from solving the 2D &amp; 3D Face A...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.07332v3.pdf</td>\n",
       "      <td>Head Pose Estimation</td>\n",
       "      <td>MAE</td>\n",
       "      <td>9.116</td>\n",
       "      <td>FAN (12 points)</td>\n",
       "      <td>https://arxiv.org/abs/1703.07332v3.pdf</td>\n",
       "      <td>Abstract:  This paper investigates how far a v...</td>\n",
       "      <td>How far are we from solving the 2D &amp; 3D Face ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Recurrent Batch Normalization</td>\n",
       "      <td>https://arxiv.org/pdf/1603.09025v5.pdf</td>\n",
       "      <td>Sequential Image Classification</td>\n",
       "      <td>Unpermuted Accuracy</td>\n",
       "      <td>99%</td>\n",
       "      <td>BN LSTM</td>\n",
       "      <td>https://arxiv.org/abs/1603.09025v5.pdf</td>\n",
       "      <td>Abstract:  We propose a reparameterization of ...</td>\n",
       "      <td>Published as a conference paper at ICLR 2017 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Neural Paraphrase Identification of Questions ...</td>\n",
       "      <td>https://arxiv.org/pdf/1704.04565v2.pdf</td>\n",
       "      <td>Paraphrase Identification</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>88.4</td>\n",
       "      <td>pt-DecAtt</td>\n",
       "      <td>https://arxiv.org/abs/1704.04565v2.pdf</td>\n",
       "      <td>Abstract:  We present a solution to the proble...</td>\n",
       "      <td>Neural Paraphrase Identification of Questions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Pushing the bounds of dropout</td>\n",
       "      <td>https://arxiv.org/pdf/1805.09208v2.pdf</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>Validation perplexity</td>\n",
       "      <td>57.1</td>\n",
       "      <td>2-layer skip-LSTM + dropout tuning</td>\n",
       "      <td>https://arxiv.org/abs/1805.09208v2.pdf</td>\n",
       "      <td>Abstract:  We show that dropout training is be...</td>\n",
       "      <td>Under review as a conference paper at ICLR 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Feedback Network for Image Super-Resolution</td>\n",
       "      <td>https://arxiv.org/pdf/1903.09814v1.pdf</td>\n",
       "      <td>Image Super-Resolution</td>\n",
       "      <td>PSNR</td>\n",
       "      <td>27.72</td>\n",
       "      <td>SRFBN</td>\n",
       "      <td>https://arxiv.org/abs/1903.09814v1.pdf</td>\n",
       "      <td>Abstract:  Recent advances in image super-reso...</td>\n",
       "      <td>Feedback Network for Image Super-Resolution Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Exploring the Limits of Language Modeling</td>\n",
       "      <td>https://arxiv.org/pdf/1602.02410v2.pdf</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>PPL</td>\n",
       "      <td>30</td>\n",
       "      <td>LSTM-8192-1024 + CNN Input</td>\n",
       "      <td>https://arxiv.org/abs/1602.02410v2.pdf</td>\n",
       "      <td>Abstract:  In this work we explore recent adva...</td>\n",
       "      <td>Exploring the Limits of Language Modeling Exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>A Large Self-Annotated Corpus for Sarcasm</td>\n",
       "      <td>https://arxiv.org/pdf/1704.05579v4.pdf</td>\n",
       "      <td>Sarcasm Detection</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>75.8</td>\n",
       "      <td>Bag-of-Bigrams</td>\n",
       "      <td>https://arxiv.org/abs/1704.05579v4.pdf</td>\n",
       "      <td>Abstract:  We introduce the Self-Annotated Red...</td>\n",
       "      <td>A Large Self-Annotated Corpus for Sarcasm Mik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Higher Order Conditional Random Fields in Deep...</td>\n",
       "      <td>https://arxiv.org/pdf/1511.08119v4.pdf</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>mIoU</td>\n",
       "      <td>41.3</td>\n",
       "      <td>HO CRF</td>\n",
       "      <td>https://arxiv.org/abs/1511.08119v4.pdf</td>\n",
       "      <td>Abstract:  We address the problem of semantic ...</td>\n",
       "      <td>Higher Order Conditional Random Fields in Dee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Learning Transferable Architectures for Scalab...</td>\n",
       "      <td>https://arxiv.org/pdf/1707.07012v4.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>Top 1 Accuracy</td>\n",
       "      <td>82.70%</td>\n",
       "      <td>NASNET-A(6)</td>\n",
       "      <td>https://arxiv.org/abs/1707.07012v4.pdf</td>\n",
       "      <td>Abstract:  Developing neural network image cla...</td>\n",
       "      <td>Learning Transferable Architectures for Scala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Read + Verify: Machine Reading Comprehension w...</td>\n",
       "      <td>https://arxiv.org/pdf/1808.05759v5.pdf</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>EM</td>\n",
       "      <td>71.767</td>\n",
       "      <td>Reinforced Mnemonic Reader + Answer Verifier (...</td>\n",
       "      <td>https://arxiv.org/abs/1808.05759v5.pdf</td>\n",
       "      <td>Abstract:  Machine reading comprehension with ...</td>\n",
       "      <td>Read + Verify: Machine Reading Comprehension ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Training Deep AutoEncoders for Collaborative F...</td>\n",
       "      <td>https://arxiv.org/pdf/1708.01715v3.pdf</td>\n",
       "      <td>Collaborative Filtering</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>0.9099</td>\n",
       "      <td>DeepRec</td>\n",
       "      <td>https://arxiv.org/abs/1708.01715v3.pdf</td>\n",
       "      <td>Abstract:  This paper proposes a novel model f...</td>\n",
       "      <td>Training Deep AutoEncoders for Collaborative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>No more meta-parameter tuning in unsupervised ...</td>\n",
       "      <td>https://arxiv.org/pdf/1402.5766v1.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>Percentage correct</td>\n",
       "      <td>61</td>\n",
       "      <td>No more meta-parameter tuning in unsupervised ...</td>\n",
       "      <td>https://arxiv.org/abs/1402.5766v1.pdf</td>\n",
       "      <td>Abstract:  We propose a meta-parameter free, o...</td>\n",
       "      <td>No more meta-parameter tuning in unsupervised...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Wide &amp; Deep Learning for Recommender Systems</td>\n",
       "      <td>https://arxiv.org/pdf/1606.07792v1.pdf</td>\n",
       "      <td>Click-Through Rate Prediction</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.8637</td>\n",
       "      <td>Wide &amp; Deep</td>\n",
       "      <td>https://arxiv.org/abs/1606.07792v1.pdf</td>\n",
       "      <td>Abstract:  Generalized linear models with nonl...</td>\n",
       "      <td>Wide &amp; Deep Learning for Recommender Systems ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Deep Learning For Smile Recognition</td>\n",
       "      <td>https://arxiv.org/pdf/1602.00172v2.pdf</td>\n",
       "      <td>Smile Recognition</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>99.45%</td>\n",
       "      <td>Deep CNN</td>\n",
       "      <td>https://arxiv.org/abs/1602.00172v2.pdf</td>\n",
       "      <td>Abstract:  Inspired by recent successes of dee...</td>\n",
       "      <td>July 26, 2017 1:47 WSPC - Proceedings Trim Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>WaveNet: A Generative Model for Raw Audio</td>\n",
       "      <td>https://arxiv.org/pdf/1609.03499v2.pdf</td>\n",
       "      <td>Speech Synthesis</td>\n",
       "      <td>Mean Opinion Score</td>\n",
       "      <td>4.08</td>\n",
       "      <td>WaveNet (L+F)</td>\n",
       "      <td>https://arxiv.org/abs/1609.03499v2.pdf</td>\n",
       "      <td>Abstract:  This paper introduces WaveNet, a de...</td>\n",
       "      <td>WAVENET: A GENERATIVE MODEL FOR RAW AUDIO Aä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Deep Joint Entity Disambiguation with Local Ne...</td>\n",
       "      <td>https://arxiv.org/pdf/1704.04920v3.pdf</td>\n",
       "      <td>Entity Resolution</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>92.22</td>\n",
       "      <td>deep joint entity disambiguation w/ neural att...</td>\n",
       "      <td>https://arxiv.org/abs/1704.04920v3.pdf</td>\n",
       "      <td>Abstract:  We propose a novel deep learning mo...</td>\n",
       "      <td>Deep Joint Entity Disambiguation with Local N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Multi-Scale Context Aggregation by Dilated Con...</td>\n",
       "      <td>https://arxiv.org/pdf/1511.07122v3.pdf</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>Validation mIoU</td>\n",
       "      <td>32.31</td>\n",
       "      <td>DilatedNet</td>\n",
       "      <td>https://arxiv.org/abs/1511.07122v3.pdf</td>\n",
       "      <td>Abstract:  State-of-the-art models for semanti...</td>\n",
       "      <td>Published as a conference paper at ICLR 2016 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Unsupervised Neural Machine Translation with S...</td>\n",
       "      <td>https://arxiv.org/pdf/1901.04112v1.pdf</td>\n",
       "      <td>Unsupervised Machine Translation</td>\n",
       "      <td>BLEU</td>\n",
       "      <td>29.5</td>\n",
       "      <td>SMT as posterior regularization</td>\n",
       "      <td>https://arxiv.org/abs/1901.04112v1.pdf</td>\n",
       "      <td>Abstract:  Without real bilingual corpus avail...</td>\n",
       "      <td>Unsupervised Neural Machine Translation with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>EnhanceNet: Single Image Super-Resolution Thro...</td>\n",
       "      <td>https://arxiv.org/pdf/1612.07919v2.pdf</td>\n",
       "      <td>Image Super-Resolution</td>\n",
       "      <td>PSNR</td>\n",
       "      <td>27.5</td>\n",
       "      <td>ENet-E</td>\n",
       "      <td>https://arxiv.org/abs/1612.07919v2.pdf</td>\n",
       "      <td>Abstract:  Single image super-resolution is th...</td>\n",
       "      <td>EnhanceNet: Single Image Super-Resolution Thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Fine-Grained Representation Learning and Recog...</td>\n",
       "      <td>https://arxiv.org/pdf/1808.04505v1.pdf</td>\n",
       "      <td>Fine-Grained Image Classification</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>88.10%</td>\n",
       "      <td>Hierarchical Semantic Embedding</td>\n",
       "      <td>https://arxiv.org/abs/1808.04505v1.pdf</td>\n",
       "      <td>Abstract:  Object categories inherently form a...</td>\n",
       "      <td>Fine-Grained Representation Learning and Reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Deep Pyramidal Residual Networks</td>\n",
       "      <td>https://arxiv.org/pdf/1610.02915v4.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>Percentage correct</td>\n",
       "      <td>96.69</td>\n",
       "      <td>Deep pyramidal residual network</td>\n",
       "      <td>https://arxiv.org/abs/1610.02915v4.pdf</td>\n",
       "      <td>Abstract:  Deep convolutional neural networks ...</td>\n",
       "      <td>Deep Pyramidal Residual Networks Dongyoon Han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Stochastic Answer Networks for Natural Languag...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.07888v2.pdf</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>88.5</td>\n",
       "      <td>Stochastic Answer Network</td>\n",
       "      <td>https://arxiv.org/abs/1804.07888v2.pdf</td>\n",
       "      <td>Abstract:  We propose a stochastic answer netw...</td>\n",
       "      <td>Stochastic Answer Networks for Natural Langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Evaluating Semantic Parsing against a Simple W...</td>\n",
       "      <td>https://arxiv.org/pdf/1707.04412v1.pdf</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>F1</td>\n",
       "      <td>53.6</td>\n",
       "      <td>WebQA</td>\n",
       "      <td>https://arxiv.org/abs/1707.04412v1.pdf</td>\n",
       "      <td>Abstract:  Semantic parsing shines at analyzin...</td>\n",
       "      <td>Evaluating Semantic Parsing against a Simple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Attending to Characters in Neural Sequence Lab...</td>\n",
       "      <td>https://arxiv.org/pdf/1611.04361v1.pdf</td>\n",
       "      <td>Grammatical Error Detection</td>\n",
       "      <td>F0.5</td>\n",
       "      <td>41.88</td>\n",
       "      <td>Bi-LSTM + charattn</td>\n",
       "      <td>https://arxiv.org/abs/1611.04361v1.pdf</td>\n",
       "      <td>Abstract:  Sequence labeling architectures use...</td>\n",
       "      <td>Attending to Characters in Neural Sequence La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>A Mention-Ranking Model for Abstract Anaphora ...</td>\n",
       "      <td>https://arxiv.org/pdf/1706.02256v2.pdf</td>\n",
       "      <td>Abstract Anaphora Resolution</td>\n",
       "      <td>Average Precision</td>\n",
       "      <td>43.83</td>\n",
       "      <td>MR-LSTM</td>\n",
       "      <td>https://arxiv.org/abs/1706.02256v2.pdf</td>\n",
       "      <td>Abstract:  Resolving abstract anaphora is an i...</td>\n",
       "      <td>A Mention-Ranking Model for Abstract Anaphora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>OverFeat: Integrated Recognition, Localization...</td>\n",
       "      <td>https://arxiv.org/pdf/1312.6229v4.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>MAP</td>\n",
       "      <td>24.30%</td>\n",
       "      <td>OverFeat</td>\n",
       "      <td>https://arxiv.org/abs/1312.6229v4.pdf</td>\n",
       "      <td>Abstract:  We present an integrated framework ...</td>\n",
       "      <td>() ar X iv :1 31 2. 62 29 v4 [ cs .C V ] 2 4 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paper_title  \\\n",
       "0   The LovÌÁsz-Softmax loss: A tractable surrogat...   \n",
       "1   Bottom-Up and Top-Down Attention for Image Cap...   \n",
       "2                                 Deep Fried Convnets   \n",
       "3   Learning Natural Language Inference using Bidi...   \n",
       "4   Cascade R-CNN: Delving into High Quality Objec...   \n",
       "5   Fast and Accurate Deep Network Learning by Exp...   \n",
       "6   Language Modeling with Gated Convolutional Net...   \n",
       "7   Pervasive Attention: 2D Convolutional Neural N...   \n",
       "8   Deep Bayesian Bandits Showdown: An Empirical C...   \n",
       "9   CoupleNet: Coupling Global Structure with Loca...   \n",
       "10  Semi-Supervised Classification with Graph Conv...   \n",
       "11  The Best of Both Worlds: Combining Recent Adva...   \n",
       "12  Brain Tumor Segmentation with Deep Neural Netw...   \n",
       "13  Conditional Image Synthesis With Auxiliary Cla...   \n",
       "14  Supervised Learning of Universal Sentence Repr...   \n",
       "15  Adversarial Training Methods for Semi-Supervis...   \n",
       "16  On the Role of Text Preprocessing in Neural Ne...   \n",
       "17  Natural TTS Synthesis by Conditioning WaveNet ...   \n",
       "18  Scale-Aware Trident Networks for Object Detection   \n",
       "19  Deeply learned face representations are sparse...   \n",
       "20  Hyperspectral Image Classification with Markov...   \n",
       "21  A Deep Relevance Matching Model for Ad-hoc Ret...   \n",
       "22  Towards a Better Match in Siamese Network Base...   \n",
       "23  Learning Discriminative Features with Multiple...   \n",
       "24  How far are we from solving the 2D & 3D Face A...   \n",
       "25                      Recurrent Batch Normalization   \n",
       "26  Neural Paraphrase Identification of Questions ...   \n",
       "27                      Pushing the bounds of dropout   \n",
       "28        Feedback Network for Image Super-Resolution   \n",
       "29          Exploring the Limits of Language Modeling   \n",
       "30          A Large Self-Annotated Corpus for Sarcasm   \n",
       "31  Higher Order Conditional Random Fields in Deep...   \n",
       "32  Learning Transferable Architectures for Scalab...   \n",
       "33  Read + Verify: Machine Reading Comprehension w...   \n",
       "34  Training Deep AutoEncoders for Collaborative F...   \n",
       "35  No more meta-parameter tuning in unsupervised ...   \n",
       "36       Wide & Deep Learning for Recommender Systems   \n",
       "37                Deep Learning For Smile Recognition   \n",
       "38          WaveNet: A Generative Model for Raw Audio   \n",
       "39  Deep Joint Entity Disambiguation with Local Ne...   \n",
       "40  Multi-Scale Context Aggregation by Dilated Con...   \n",
       "41  Unsupervised Neural Machine Translation with S...   \n",
       "42  EnhanceNet: Single Image Super-Resolution Thro...   \n",
       "43  Fine-Grained Representation Learning and Recog...   \n",
       "44                   Deep Pyramidal Residual Networks   \n",
       "45  Stochastic Answer Networks for Natural Languag...   \n",
       "46  Evaluating Semantic Parsing against a Simple W...   \n",
       "47  Attending to Characters in Neural Sequence Lab...   \n",
       "48  A Mention-Ranking Model for Abstract Anaphora ...   \n",
       "49  OverFeat: Integrated Recognition, Localization...   \n",
       "\n",
       "                                 paper_url  \\\n",
       "0   https://arxiv.org/pdf/1705.08790v2.pdf   \n",
       "1   https://arxiv.org/pdf/1707.07998v3.pdf   \n",
       "2    https://arxiv.org/pdf/1412.7149v4.pdf   \n",
       "3   https://arxiv.org/pdf/1605.09090v1.pdf   \n",
       "4   https://arxiv.org/pdf/1712.00726v1.pdf   \n",
       "5   https://arxiv.org/pdf/1511.07289v5.pdf   \n",
       "6   https://arxiv.org/pdf/1612.08083v3.pdf   \n",
       "7   https://arxiv.org/pdf/1808.03867v3.pdf   \n",
       "8   https://arxiv.org/pdf/1802.09127v1.pdf   \n",
       "9   https://arxiv.org/pdf/1708.02863v1.pdf   \n",
       "10  https://arxiv.org/pdf/1609.02907v4.pdf   \n",
       "11  https://arxiv.org/pdf/1804.09849v2.pdf   \n",
       "12  https://arxiv.org/pdf/1505.03540v3.pdf   \n",
       "13  https://arxiv.org/pdf/1610.09585v4.pdf   \n",
       "14  https://arxiv.org/pdf/1705.02364v5.pdf   \n",
       "15  https://arxiv.org/pdf/1605.07725v3.pdf   \n",
       "16  https://arxiv.org/pdf/1707.01780v3.pdf   \n",
       "17  https://arxiv.org/pdf/1712.05884v2.pdf   \n",
       "18  https://arxiv.org/pdf/1901.01892v1.pdf   \n",
       "19   https://arxiv.org/pdf/1412.1265v1.pdf   \n",
       "20  https://arxiv.org/pdf/1705.00727v2.pdf   \n",
       "21  https://arxiv.org/pdf/1711.08611v1.pdf   \n",
       "22  https://arxiv.org/pdf/1809.01368v1.pdf   \n",
       "23  https://arxiv.org/pdf/1804.01438v3.pdf   \n",
       "24  https://arxiv.org/pdf/1703.07332v3.pdf   \n",
       "25  https://arxiv.org/pdf/1603.09025v5.pdf   \n",
       "26  https://arxiv.org/pdf/1704.04565v2.pdf   \n",
       "27  https://arxiv.org/pdf/1805.09208v2.pdf   \n",
       "28  https://arxiv.org/pdf/1903.09814v1.pdf   \n",
       "29  https://arxiv.org/pdf/1602.02410v2.pdf   \n",
       "30  https://arxiv.org/pdf/1704.05579v4.pdf   \n",
       "31  https://arxiv.org/pdf/1511.08119v4.pdf   \n",
       "32  https://arxiv.org/pdf/1707.07012v4.pdf   \n",
       "33  https://arxiv.org/pdf/1808.05759v5.pdf   \n",
       "34  https://arxiv.org/pdf/1708.01715v3.pdf   \n",
       "35   https://arxiv.org/pdf/1402.5766v1.pdf   \n",
       "36  https://arxiv.org/pdf/1606.07792v1.pdf   \n",
       "37  https://arxiv.org/pdf/1602.00172v2.pdf   \n",
       "38  https://arxiv.org/pdf/1609.03499v2.pdf   \n",
       "39  https://arxiv.org/pdf/1704.04920v3.pdf   \n",
       "40  https://arxiv.org/pdf/1511.07122v3.pdf   \n",
       "41  https://arxiv.org/pdf/1901.04112v1.pdf   \n",
       "42  https://arxiv.org/pdf/1612.07919v2.pdf   \n",
       "43  https://arxiv.org/pdf/1808.04505v1.pdf   \n",
       "44  https://arxiv.org/pdf/1610.02915v4.pdf   \n",
       "45  https://arxiv.org/pdf/1804.07888v2.pdf   \n",
       "46  https://arxiv.org/pdf/1707.04412v1.pdf   \n",
       "47  https://arxiv.org/pdf/1611.04361v1.pdf   \n",
       "48  https://arxiv.org/pdf/1706.02256v2.pdf   \n",
       "49   https://arxiv.org/pdf/1312.6229v4.pdf   \n",
       "\n",
       "                                  task                     metric_name  \\\n",
       "0                Semantic Segmentation                        Mean IoU   \n",
       "1            Visual Question Answering              Percentage correct   \n",
       "2                 Image Classification                Percentage error   \n",
       "3           Natural Language Inference                 % Test Accuracy   \n",
       "4                     Object Detection                 Bounding Box AP   \n",
       "5                 Image Classification              Percentage correct   \n",
       "6                   Language Modelling                             PPL   \n",
       "7                  Machine Translation                      BLEU score   \n",
       "8                  Multi-Armed Bandits               Cumulative regret   \n",
       "9                     Object Detection                             MAP   \n",
       "10                 Node Classification                        Accuracy   \n",
       "11                 Machine Translation                      BLEU score   \n",
       "12            Brain Tumor Segmentation                      Dice Score   \n",
       "13        Conditional Image Generation                 Inception score   \n",
       "14         Semantic Textual Similarity                            MRPC   \n",
       "15                  Sentiment Analysis                        Accuracy   \n",
       "16                  Sentiment Analysis                        Accuracy   \n",
       "17                    Speech Synthesis              Mean Opinion Score   \n",
       "18                    Object Detection                 Bounding Box AP   \n",
       "19                   Face Verification                        Accuracy   \n",
       "20  Hyperspectral Image Classification                Overall Accuracy   \n",
       "21        Ad-Hoc Information Retrieval                             MAP   \n",
       "22              Visual Object Tracking  Expected Average Overlap (EAO)   \n",
       "23            Person Re-Identification                          Rank-1   \n",
       "24                Head Pose Estimation                             MAE   \n",
       "25     Sequential Image Classification             Unpermuted Accuracy   \n",
       "26           Paraphrase Identification                        Accuracy   \n",
       "27                  Language Modelling           Validation perplexity   \n",
       "28              Image Super-Resolution                            PSNR   \n",
       "29                  Language Modelling                             PPL   \n",
       "30                   Sarcasm Detection                        Accuracy   \n",
       "31               Semantic Segmentation                            mIoU   \n",
       "32                Image Classification                  Top 1 Accuracy   \n",
       "33                  Question Answering                              EM   \n",
       "34             Collaborative Filtering                            RMSE   \n",
       "35                Image Classification              Percentage correct   \n",
       "36       Click-Through Rate Prediction                             AUC   \n",
       "37                   Smile Recognition                        Accuracy   \n",
       "38                    Speech Synthesis              Mean Opinion Score   \n",
       "39                   Entity Resolution                        Accuracy   \n",
       "40               Semantic Segmentation                 Validation mIoU   \n",
       "41    Unsupervised Machine Translation                            BLEU   \n",
       "42              Image Super-Resolution                            PSNR   \n",
       "43   Fine-Grained Image Classification                        Accuracy   \n",
       "44                Image Classification              Percentage correct   \n",
       "45          Natural Language Inference                 % Test Accuracy   \n",
       "46                  Question Answering                              F1   \n",
       "47         Grammatical Error Detection                            F0.5   \n",
       "48        Abstract Anaphora Resolution               Average Precision   \n",
       "49                    Object Detection                             MAP   \n",
       "\n",
       "   metric_value                                              model  \\\n",
       "0        79.00%                Deeplab-v2 with Lovasz-Softmax loss   \n",
       "1         70.34                                            Up-Down   \n",
       "2           0.7                                Deep Fried Convnets   \n",
       "3          83.3                     600D (300+300) BiLSTM encoders   \n",
       "4          42.8                                      Cascade R-CNN   \n",
       "5          93.5                           Exponential Linear Units   \n",
       "6          31.9                                 GCNN-14 bottleneck   \n",
       "7         27.99                                Pervasive Attention   \n",
       "8          1.92                      NeuralLinear FullPosterior-MR   \n",
       "9        82.70%                                          CoupleNet   \n",
       "10       70.30%                                                GCN   \n",
       "11        41.0*                                              RNMT+   \n",
       "12         0.88                                    InputCascadeCNN   \n",
       "13         8.25                                             AC-GAN   \n",
       "14    76.2/83.1                                          InferSent   \n",
       "15         94.1                       Virtual adversarial training   \n",
       "16         88.9                                           CNN+LSTM   \n",
       "17        4.526                                         Tacotron 2   \n",
       "18         48.4                                         TridentNet   \n",
       "19       99.47%                                           DeepId2+   \n",
       "20       96.12%                                            CNN-MRF   \n",
       "21        0.279                                               DRMM   \n",
       "22        0.337                                          SA Siam R   \n",
       "23         88.7                                                MGN   \n",
       "24        9.116                                    FAN (12 points)   \n",
       "25          99%                                            BN LSTM   \n",
       "26         88.4                                          pt-DecAtt   \n",
       "27         57.1                 2-layer skip-LSTM + dropout tuning   \n",
       "28        27.72                                              SRFBN   \n",
       "29           30                         LSTM-8192-1024 + CNN Input   \n",
       "30         75.8                                     Bag-of-Bigrams   \n",
       "31         41.3                                             HO CRF   \n",
       "32       82.70%                                        NASNET-A(6)   \n",
       "33       71.767  Reinforced Mnemonic Reader + Answer Verifier (...   \n",
       "34       0.9099                                            DeepRec   \n",
       "35           61  No more meta-parameter tuning in unsupervised ...   \n",
       "36       0.8637                                        Wide & Deep   \n",
       "37       99.45%                                           Deep CNN   \n",
       "38         4.08                                      WaveNet (L+F)   \n",
       "39        92.22  deep joint entity disambiguation w/ neural att...   \n",
       "40        32.31                                         DilatedNet   \n",
       "41         29.5                    SMT as posterior regularization   \n",
       "42         27.5                                             ENet-E   \n",
       "43       88.10%                    Hierarchical Semantic Embedding   \n",
       "44        96.69                    Deep pyramidal residual network   \n",
       "45         88.5                          Stochastic Answer Network   \n",
       "46         53.6                                              WebQA   \n",
       "47        41.88                                 Bi-LSTM + charattn   \n",
       "48        43.83                                            MR-LSTM   \n",
       "49       24.30%                                           OverFeat   \n",
       "\n",
       "                                 paper_abs  \\\n",
       "0   https://arxiv.org/abs/1705.08790v2.pdf   \n",
       "1   https://arxiv.org/abs/1707.07998v3.pdf   \n",
       "2    https://arxiv.org/abs/1412.7149v4.pdf   \n",
       "3   https://arxiv.org/abs/1605.09090v1.pdf   \n",
       "4   https://arxiv.org/abs/1712.00726v1.pdf   \n",
       "5   https://arxiv.org/abs/1511.07289v5.pdf   \n",
       "6   https://arxiv.org/abs/1612.08083v3.pdf   \n",
       "7   https://arxiv.org/abs/1808.03867v3.pdf   \n",
       "8   https://arxiv.org/abs/1802.09127v1.pdf   \n",
       "9   https://arxiv.org/abs/1708.02863v1.pdf   \n",
       "10  https://arxiv.org/abs/1609.02907v4.pdf   \n",
       "11  https://arxiv.org/abs/1804.09849v2.pdf   \n",
       "12  https://arxiv.org/abs/1505.03540v3.pdf   \n",
       "13  https://arxiv.org/abs/1610.09585v4.pdf   \n",
       "14  https://arxiv.org/abs/1705.02364v5.pdf   \n",
       "15  https://arxiv.org/abs/1605.07725v3.pdf   \n",
       "16  https://arxiv.org/abs/1707.01780v3.pdf   \n",
       "17  https://arxiv.org/abs/1712.05884v2.pdf   \n",
       "18  https://arxiv.org/abs/1901.01892v1.pdf   \n",
       "19   https://arxiv.org/abs/1412.1265v1.pdf   \n",
       "20  https://arxiv.org/abs/1705.00727v2.pdf   \n",
       "21  https://arxiv.org/abs/1711.08611v1.pdf   \n",
       "22  https://arxiv.org/abs/1809.01368v1.pdf   \n",
       "23  https://arxiv.org/abs/1804.01438v3.pdf   \n",
       "24  https://arxiv.org/abs/1703.07332v3.pdf   \n",
       "25  https://arxiv.org/abs/1603.09025v5.pdf   \n",
       "26  https://arxiv.org/abs/1704.04565v2.pdf   \n",
       "27  https://arxiv.org/abs/1805.09208v2.pdf   \n",
       "28  https://arxiv.org/abs/1903.09814v1.pdf   \n",
       "29  https://arxiv.org/abs/1602.02410v2.pdf   \n",
       "30  https://arxiv.org/abs/1704.05579v4.pdf   \n",
       "31  https://arxiv.org/abs/1511.08119v4.pdf   \n",
       "32  https://arxiv.org/abs/1707.07012v4.pdf   \n",
       "33  https://arxiv.org/abs/1808.05759v5.pdf   \n",
       "34  https://arxiv.org/abs/1708.01715v3.pdf   \n",
       "35   https://arxiv.org/abs/1402.5766v1.pdf   \n",
       "36  https://arxiv.org/abs/1606.07792v1.pdf   \n",
       "37  https://arxiv.org/abs/1602.00172v2.pdf   \n",
       "38  https://arxiv.org/abs/1609.03499v2.pdf   \n",
       "39  https://arxiv.org/abs/1704.04920v3.pdf   \n",
       "40  https://arxiv.org/abs/1511.07122v3.pdf   \n",
       "41  https://arxiv.org/abs/1901.04112v1.pdf   \n",
       "42  https://arxiv.org/abs/1612.07919v2.pdf   \n",
       "43  https://arxiv.org/abs/1808.04505v1.pdf   \n",
       "44  https://arxiv.org/abs/1610.02915v4.pdf   \n",
       "45  https://arxiv.org/abs/1804.07888v2.pdf   \n",
       "46  https://arxiv.org/abs/1707.04412v1.pdf   \n",
       "47  https://arxiv.org/abs/1611.04361v1.pdf   \n",
       "48  https://arxiv.org/abs/1706.02256v2.pdf   \n",
       "49   https://arxiv.org/abs/1312.6229v4.pdf   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Abstract:  The Jaccard index, also referred to...   \n",
       "1   Abstract:  Top-down visual attention mechanism...   \n",
       "2   Abstract:  The fully connected layers of a dee...   \n",
       "3   Abstract:  In this paper, we proposed a senten...   \n",
       "4   Abstract:  In object detection, an intersectio...   \n",
       "5   Abstract:  We introduce the \"exponential linea...   \n",
       "6   Abstract:  The pre-dominant approach to langua...   \n",
       "7   Abstract:  Current state-of-the-art machine tr...   \n",
       "8   Abstract:  Recent advances in deep reinforceme...   \n",
       "9   Abstract:  The region-based Convolutional Neur...   \n",
       "10  Abstract:  We present a scalable approach for ...   \n",
       "11  Abstract:  The past year has witnessed rapid a...   \n",
       "12  Abstract:  In this paper, we present a fully a...   \n",
       "13  Abstract:  Synthesizing high resolution photor...   \n",
       "14  Abstract:  Many modern NLP systems rely on wor...   \n",
       "15  Abstract:  Adversarial training provides a mea...   \n",
       "16  Abstract:  Text preprocessing is often the fir...   \n",
       "17  Abstract:  This paper describes Tacotron 2, a ...   \n",
       "18  Abstract:  Scale variation is one of the key c...   \n",
       "19  Abstract:  This paper designs a high-performan...   \n",
       "20  Abstract:  This paper presents a new supervise...   \n",
       "21  Abstract:  In recent years, deep neural networ...   \n",
       "22  Abstract:  Recently, Siamese network based tra...   \n",
       "23  Abstract:  The combination of global and parti...   \n",
       "24  Abstract:  This paper investigates how far a v...   \n",
       "25  Abstract:  We propose a reparameterization of ...   \n",
       "26  Abstract:  We present a solution to the proble...   \n",
       "27  Abstract:  We show that dropout training is be...   \n",
       "28  Abstract:  Recent advances in image super-reso...   \n",
       "29  Abstract:  In this work we explore recent adva...   \n",
       "30  Abstract:  We introduce the Self-Annotated Red...   \n",
       "31  Abstract:  We address the problem of semantic ...   \n",
       "32  Abstract:  Developing neural network image cla...   \n",
       "33  Abstract:  Machine reading comprehension with ...   \n",
       "34  Abstract:  This paper proposes a novel model f...   \n",
       "35  Abstract:  We propose a meta-parameter free, o...   \n",
       "36  Abstract:  Generalized linear models with nonl...   \n",
       "37  Abstract:  Inspired by recent successes of dee...   \n",
       "38  Abstract:  This paper introduces WaveNet, a de...   \n",
       "39  Abstract:  We propose a novel deep learning mo...   \n",
       "40  Abstract:  State-of-the-art models for semanti...   \n",
       "41  Abstract:  Without real bilingual corpus avail...   \n",
       "42  Abstract:  Single image super-resolution is th...   \n",
       "43  Abstract:  Object categories inherently form a...   \n",
       "44  Abstract:  Deep convolutional neural networks ...   \n",
       "45  Abstract:  We propose a stochastic answer netw...   \n",
       "46  Abstract:  Semantic parsing shines at analyzin...   \n",
       "47  Abstract:  Sequence labeling architectures use...   \n",
       "48  Abstract:  Resolving abstract anaphora is an i...   \n",
       "49  Abstract:  We present an integrated framework ...   \n",
       "\n",
       "                                           paper_text  \n",
       "0    Accepted as a conference paper at CVPR 2018 T...  \n",
       "1    Bottom-Up and Top-Down Attention for Image Ca...  \n",
       "2    Deep Fried Convnets Zichao Yang1 Marcin Moczu...  \n",
       "3    () ar X iv :1 60 5. 09 09 0v 1 [ cs .C L ] 3 ...  \n",
       "4    ar X iv :1 71 2. 00 72 6v 1 [ cs .C V ] 3 D e...  \n",
       "5    Published as a conference paper at ICLR 2016 ...  \n",
       "6    Language Modeling with Gated Convolutional Ne...  \n",
       "7    Pervasive Attention: 2D Convolutional Neural ...  \n",
       "8    Published as a conference paper at ICLR 2018 ...  \n",
       "9    CoupleNet: Coupling Global Structure with Loc...  \n",
       "10   Published as a conference paper at ICLR 2017 ...  \n",
       "11   The Best of Both Worlds: Combining Recent Adv...  \n",
       "12   Brain Tumor Segmentation with Deep Neural Net...  \n",
       "13   Conditional Image Synthesis with Auxiliary Cl...  \n",
       "14   ar X iv :1 70 5. 02 36 4v 5 [ cs .C L ] 8 J u...  \n",
       "15   () ar X iv :1 60 5. 07 72 5v 3 [ st at .M L ]...  \n",
       "16   ar X iv :1 70 7. 01 78 0v 3 [ cs .C L ] 2 3 A...  \n",
       "17   NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET...  \n",
       "18   Scale-Aware Trident Networks for Object Detec...  \n",
       "19   Deeply learned face representations are spars...  \n",
       "20   1 Hyperspectral Image Classification with Mar...  \n",
       "21   ar X iv :1 71 1. 08 61 1v 1 [ cs .I R ] 2 3 N...  \n",
       "22   Towards a Better Match in Siamese Network Bas...  \n",
       "23   Learning Discriminative Features with Multipl...  \n",
       "24   How far are we from solving the 2D & 3D Face ...  \n",
       "25   Published as a conference paper at ICLR 2017 ...  \n",
       "26   Neural Paraphrase Identification of Questions...  \n",
       "27   Under review as a conference paper at ICLR 20...  \n",
       "28   Feedback Network for Image Super-Resolution Z...  \n",
       "29   Exploring the Limits of Language Modeling Exp...  \n",
       "30   A Large Self-Annotated Corpus for Sarcasm Mik...  \n",
       "31   Higher Order Conditional Random Fields in Dee...  \n",
       "32   Learning Transferable Architectures for Scala...  \n",
       "33   Read + Verify: Machine Reading Comprehension ...  \n",
       "34   Training Deep AutoEncoders for Collaborative ...  \n",
       "35   No more meta-parameter tuning in unsupervised...  \n",
       "36   Wide & Deep Learning for Recommender Systems ...  \n",
       "37   July 26, 2017 1:47 WSPC - Proceedings Trim Si...  \n",
       "38   WAVENET: A GENERATIVE MODEL FOR RAW AUDIO Aä...  \n",
       "39   Deep Joint Entity Disambiguation with Local N...  \n",
       "40   Published as a conference paper at ICLR 2016 ...  \n",
       "41   Unsupervised Neural Machine Translation with ...  \n",
       "42   EnhanceNet: Single Image Super-Resolution Thr...  \n",
       "43   Fine-Grained Representation Learning and Reco...  \n",
       "44   Deep Pyramidal Residual Networks Dongyoon Han...  \n",
       "45   Stochastic Answer Networks for Natural Langua...  \n",
       "46   Evaluating Semantic Parsing against a Simple ...  \n",
       "47   Attending to Characters in Neural Sequence La...  \n",
       "48   A Mention-Ranking Model for Abstract Anaphora...  \n",
       "49   () ar X iv :1 31 2. 62 29 v4 [ cs .C V ] 2 4 ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>paper_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>https://arxiv.org/abs/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  extradata global_rank metric_name metric_value    model remove  \\\n",
       "0    IC15        NaN        # 10   F-Measure       75.61%  SegLink      -   \n",
       "\n",
       "                   task                                        paper_title  \\\n",
       "0  Scene Text Detection  Detecting Oriented Text in Natural Images by L...   \n",
       "\n",
       "                                          paper_path  \\\n",
       "0  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "\n",
       "                                paper_url  \\\n",
       "0  https://arxiv.org/pdf/1703.06520v3.pdf   \n",
       "\n",
       "                                paper_abs  \n",
       "0  https://arxiv.org/abs/1703.06520v3.pdf  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv('../data/pw_code_model_data.csv', encoding='latin-1')\n",
    "\n",
    "# df = df_raw[['paper_url', 'metric_name', 'metric_value', 'task', 'dataset', 'model', 'paper_title']]\n",
    "\n",
    "# drop duplicate papers\n",
    "df_2 = df_2.drop_duplicates(subset='paper_url')\n",
    "\n",
    "df_2 = df_2[df_2['paper_url'].str.contains(\"arxiv\")]\n",
    "\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "\n",
    "df_2['paper_abs'] = df_2.paper_url.str.replace('/pdf', '/abs')\n",
    "\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_value</th>\n",
       "      <th>dataset</th>\n",
       "      <th>metric_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>79.00%</td>\n",
       "      <td>PASCAL VOC 2012</td>\n",
       "      <td>Mean IoU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>70.34</td>\n",
       "      <td>COCO Visual Question Answering (VQA) real imag...</td>\n",
       "      <td>Percentage correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.7</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>Percentage error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>83.3</td>\n",
       "      <td>SNLI</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>42.8</td>\n",
       "      <td>COCO</td>\n",
       "      <td>Bounding Box AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    metric_value                                            dataset  \\\n",
       "200       79.00%                                    PASCAL VOC 2012   \n",
       "201        70.34  COCO Visual Question Answering (VQA) real imag...   \n",
       "202          0.7                                              MNIST   \n",
       "203         83.3                                               SNLI   \n",
       "204         42.8                                               COCO   \n",
       "\n",
       "            metric_name  \n",
       "200            Mean IoU  \n",
       "201  Percentage correct  \n",
       "202    Percentage error  \n",
       "203     % Test Accuracy  \n",
       "204     Bounding Box AP  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = df_2[200:300]\n",
    "\n",
    "df_2 = df_2[['metric_value', 'dataset', 'metric_name']]\n",
    "\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_3 = pd.merge(df, df_2, on=['metric_value', 'metric_name'])\n",
    "\n",
    "df_3 = df_3[['paper_title', 'paper_url', 'task', 'dataset', 'metric_name', 'metric_value', 'model', 'paper_abs', 'abstract', 'paper_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>task</th>\n",
       "      <th>dataset</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>paper_abs</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The LovÌÁsz-Softmax loss: A tractable surrogat...</td>\n",
       "      <td>https://arxiv.org/pdf/1705.08790v2.pdf</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>PASCAL VOC 2012</td>\n",
       "      <td>Mean IoU</td>\n",
       "      <td>79.00%</td>\n",
       "      <td>Deeplab-v2 with Lovasz-Softmax loss</td>\n",
       "      <td>https://arxiv.org/abs/1705.08790v2.pdf</td>\n",
       "      <td>Abstract:  The Jaccard index, also referred to...</td>\n",
       "      <td>Accepted as a conference paper at CVPR 2018 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bottom-Up and Top-Down Attention for Image Cap...</td>\n",
       "      <td>https://arxiv.org/pdf/1707.07998v3.pdf</td>\n",
       "      <td>Visual Question Answering</td>\n",
       "      <td>COCO Visual Question Answering (VQA) real imag...</td>\n",
       "      <td>Percentage correct</td>\n",
       "      <td>70.34</td>\n",
       "      <td>Up-Down</td>\n",
       "      <td>https://arxiv.org/abs/1707.07998v3.pdf</td>\n",
       "      <td>Abstract:  Top-down visual attention mechanism...</td>\n",
       "      <td>Bottom-Up and Top-Down Attention for Image Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Fried Convnets</td>\n",
       "      <td>https://arxiv.org/pdf/1412.7149v4.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>Percentage error</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Deep Fried Convnets</td>\n",
       "      <td>https://arxiv.org/abs/1412.7149v4.pdf</td>\n",
       "      <td>Abstract:  The fully connected layers of a dee...</td>\n",
       "      <td>Deep Fried Convnets Zichao Yang1 Marcin Moczu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Learning Natural Language Inference using Bidi...</td>\n",
       "      <td>https://arxiv.org/pdf/1605.09090v1.pdf</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>SNLI</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>83.3</td>\n",
       "      <td>600D (300+300) BiLSTM encoders</td>\n",
       "      <td>https://arxiv.org/abs/1605.09090v1.pdf</td>\n",
       "      <td>Abstract:  In this paper, we proposed a senten...</td>\n",
       "      <td>() ar X iv :1 60 5. 09 09 0v 1 [ cs .C L ] 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cascade R-CNN: Delving into High Quality Objec...</td>\n",
       "      <td>https://arxiv.org/pdf/1712.00726v1.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>COCO</td>\n",
       "      <td>Bounding Box AP</td>\n",
       "      <td>42.8</td>\n",
       "      <td>Cascade R-CNN</td>\n",
       "      <td>https://arxiv.org/abs/1712.00726v1.pdf</td>\n",
       "      <td>Abstract:  In object detection, an intersectio...</td>\n",
       "      <td>ar X iv :1 71 2. 00 72 6v 1 [ cs .C V ] 3 D e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fast and Accurate Deep Network Learning by Exp...</td>\n",
       "      <td>https://arxiv.org/pdf/1511.07289v5.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Percentage correct</td>\n",
       "      <td>93.5</td>\n",
       "      <td>Exponential Linear Units</td>\n",
       "      <td>https://arxiv.org/abs/1511.07289v5.pdf</td>\n",
       "      <td>Abstract:  We introduce the \"exponential linea...</td>\n",
       "      <td>Published as a conference paper at ICLR 2016 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Language Modeling with Gated Convolutional Net...</td>\n",
       "      <td>https://arxiv.org/pdf/1612.08083v3.pdf</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>One Billion Word</td>\n",
       "      <td>PPL</td>\n",
       "      <td>31.9</td>\n",
       "      <td>GCNN-14 bottleneck</td>\n",
       "      <td>https://arxiv.org/abs/1612.08083v3.pdf</td>\n",
       "      <td>Abstract:  The pre-dominant approach to langua...</td>\n",
       "      <td>Language Modeling with Gated Convolutional Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pervasive Attention: 2D Convolutional Neural N...</td>\n",
       "      <td>https://arxiv.org/pdf/1808.03867v3.pdf</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>IWSLT2015 English-German</td>\n",
       "      <td>BLEU score</td>\n",
       "      <td>27.99</td>\n",
       "      <td>Pervasive Attention</td>\n",
       "      <td>https://arxiv.org/abs/1808.03867v3.pdf</td>\n",
       "      <td>Abstract:  Current state-of-the-art machine tr...</td>\n",
       "      <td>Pervasive Attention: 2D Convolutional Neural ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Deep Bayesian Bandits Showdown: An Empirical C...</td>\n",
       "      <td>https://arxiv.org/pdf/1802.09127v1.pdf</td>\n",
       "      <td>Multi-Armed Bandits</td>\n",
       "      <td>Mushroom</td>\n",
       "      <td>Cumulative regret</td>\n",
       "      <td>1.92</td>\n",
       "      <td>NeuralLinear FullPosterior-MR</td>\n",
       "      <td>https://arxiv.org/abs/1802.09127v1.pdf</td>\n",
       "      <td>Abstract:  Recent advances in deep reinforceme...</td>\n",
       "      <td>Published as a conference paper at ICLR 2018 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CoupleNet: Coupling Global Structure with Loca...</td>\n",
       "      <td>https://arxiv.org/pdf/1708.02863v1.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>PASCAL VOC 2007</td>\n",
       "      <td>MAP</td>\n",
       "      <td>82.70%</td>\n",
       "      <td>CoupleNet</td>\n",
       "      <td>https://arxiv.org/abs/1708.02863v1.pdf</td>\n",
       "      <td>Abstract:  The region-based Convolutional Neur...</td>\n",
       "      <td>CoupleNet: Coupling Global Structure with Loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Semi-Supervised Classification with Graph Conv...</td>\n",
       "      <td>https://arxiv.org/pdf/1609.02907v4.pdf</td>\n",
       "      <td>Node Classification</td>\n",
       "      <td>Citeseer</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>70.30%</td>\n",
       "      <td>GCN</td>\n",
       "      <td>https://arxiv.org/abs/1609.02907v4.pdf</td>\n",
       "      <td>Abstract:  We present a scalable approach for ...</td>\n",
       "      <td>Published as a conference paper at ICLR 2017 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Best of Both Worlds: Combining Recent Adva...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.09849v2.pdf</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>WMT2014 English-French</td>\n",
       "      <td>BLEU score</td>\n",
       "      <td>41.0*</td>\n",
       "      <td>RNMT+</td>\n",
       "      <td>https://arxiv.org/abs/1804.09849v2.pdf</td>\n",
       "      <td>Abstract:  The past year has witnessed rapid a...</td>\n",
       "      <td>The Best of Both Worlds: Combining Recent Adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Brain Tumor Segmentation with Deep Neural Netw...</td>\n",
       "      <td>https://arxiv.org/pdf/1505.03540v3.pdf</td>\n",
       "      <td>Brain Tumor Segmentation</td>\n",
       "      <td>BRATS-2013</td>\n",
       "      <td>Dice Score</td>\n",
       "      <td>0.88</td>\n",
       "      <td>InputCascadeCNN</td>\n",
       "      <td>https://arxiv.org/abs/1505.03540v3.pdf</td>\n",
       "      <td>Abstract:  In this paper, we present a fully a...</td>\n",
       "      <td>Brain Tumor Segmentation with Deep Neural Net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Conditional Image Synthesis With Auxiliary Cla...</td>\n",
       "      <td>https://arxiv.org/pdf/1610.09585v4.pdf</td>\n",
       "      <td>Conditional Image Generation</td>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Inception score</td>\n",
       "      <td>8.25</td>\n",
       "      <td>AC-GAN</td>\n",
       "      <td>https://arxiv.org/abs/1610.09585v4.pdf</td>\n",
       "      <td>Abstract:  Synthesizing high resolution photor...</td>\n",
       "      <td>Conditional Image Synthesis with Auxiliary Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Supervised Learning of Universal Sentence Repr...</td>\n",
       "      <td>https://arxiv.org/pdf/1705.02364v5.pdf</td>\n",
       "      <td>Semantic Textual Similarity</td>\n",
       "      <td>SentEval</td>\n",
       "      <td>MRPC</td>\n",
       "      <td>76.2/83.1</td>\n",
       "      <td>InferSent</td>\n",
       "      <td>https://arxiv.org/abs/1705.02364v5.pdf</td>\n",
       "      <td>Abstract:  Many modern NLP systems rely on wor...</td>\n",
       "      <td>ar X iv :1 70 5. 02 36 4v 5 [ cs .C L ] 8 J u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Adversarial Training Methods for Semi-Supervis...</td>\n",
       "      <td>https://arxiv.org/pdf/1605.07725v3.pdf</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>IMDb</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>94.1</td>\n",
       "      <td>Virtual adversarial training</td>\n",
       "      <td>https://arxiv.org/abs/1605.07725v3.pdf</td>\n",
       "      <td>Abstract:  Adversarial training provides a mea...</td>\n",
       "      <td>() ar X iv :1 60 5. 07 72 5v 3 [ st at .M L ]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>On the Role of Text Preprocessing in Neural Ne...</td>\n",
       "      <td>https://arxiv.org/pdf/1707.01780v3.pdf</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>IMDb</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>88.9</td>\n",
       "      <td>CNN+LSTM</td>\n",
       "      <td>https://arxiv.org/abs/1707.01780v3.pdf</td>\n",
       "      <td>Abstract:  Text preprocessing is often the fir...</td>\n",
       "      <td>ar X iv :1 70 7. 01 78 0v 3 [ cs .C L ] 2 3 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Natural TTS Synthesis by Conditioning WaveNet ...</td>\n",
       "      <td>https://arxiv.org/pdf/1712.05884v2.pdf</td>\n",
       "      <td>Speech Synthesis</td>\n",
       "      <td>North American English</td>\n",
       "      <td>Mean Opinion Score</td>\n",
       "      <td>4.526</td>\n",
       "      <td>Tacotron 2</td>\n",
       "      <td>https://arxiv.org/abs/1712.05884v2.pdf</td>\n",
       "      <td>Abstract:  This paper describes Tacotron 2, a ...</td>\n",
       "      <td>NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Scale-Aware Trident Networks for Object Detection</td>\n",
       "      <td>https://arxiv.org/pdf/1901.01892v1.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>COCO</td>\n",
       "      <td>Bounding Box AP</td>\n",
       "      <td>48.4</td>\n",
       "      <td>TridentNet</td>\n",
       "      <td>https://arxiv.org/abs/1901.01892v1.pdf</td>\n",
       "      <td>Abstract:  Scale variation is one of the key c...</td>\n",
       "      <td>Scale-Aware Trident Networks for Object Detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Deeply learned face representations are sparse...</td>\n",
       "      <td>https://arxiv.org/pdf/1412.1265v1.pdf</td>\n",
       "      <td>Face Verification</td>\n",
       "      <td>Labeled Faces in the Wild</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>99.47%</td>\n",
       "      <td>DeepId2+</td>\n",
       "      <td>https://arxiv.org/abs/1412.1265v1.pdf</td>\n",
       "      <td>Abstract:  This paper designs a high-performan...</td>\n",
       "      <td>Deeply learned face representations are spars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hyperspectral Image Classification with Markov...</td>\n",
       "      <td>https://arxiv.org/pdf/1705.00727v2.pdf</td>\n",
       "      <td>Hyperspectral Image Classification</td>\n",
       "      <td>Indian Pines</td>\n",
       "      <td>Overall Accuracy</td>\n",
       "      <td>96.12%</td>\n",
       "      <td>CNN-MRF</td>\n",
       "      <td>https://arxiv.org/abs/1705.00727v2.pdf</td>\n",
       "      <td>Abstract:  This paper presents a new supervise...</td>\n",
       "      <td>1 Hyperspectral Image Classification with Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A Deep Relevance Matching Model for Ad-hoc Ret...</td>\n",
       "      <td>https://arxiv.org/pdf/1711.08611v1.pdf</td>\n",
       "      <td>Ad-Hoc Information Retrieval</td>\n",
       "      <td>TREC Robust04</td>\n",
       "      <td>MAP</td>\n",
       "      <td>0.279</td>\n",
       "      <td>DRMM</td>\n",
       "      <td>https://arxiv.org/abs/1711.08611v1.pdf</td>\n",
       "      <td>Abstract:  In recent years, deep neural networ...</td>\n",
       "      <td>ar X iv :1 71 1. 08 61 1v 1 [ cs .I R ] 2 3 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Towards a Better Match in Siamese Network Base...</td>\n",
       "      <td>https://arxiv.org/pdf/1809.01368v1.pdf</td>\n",
       "      <td>Visual Object Tracking</td>\n",
       "      <td>VOT2017/18</td>\n",
       "      <td>Expected Average Overlap (EAO)</td>\n",
       "      <td>0.337</td>\n",
       "      <td>SA Siam R</td>\n",
       "      <td>https://arxiv.org/abs/1809.01368v1.pdf</td>\n",
       "      <td>Abstract:  Recently, Siamese network based tra...</td>\n",
       "      <td>Towards a Better Match in Siamese Network Bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Learning Discriminative Features with Multiple...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.01438v3.pdf</td>\n",
       "      <td>Person Re-Identification</td>\n",
       "      <td>DukeMTMC-reID</td>\n",
       "      <td>Rank-1</td>\n",
       "      <td>88.7</td>\n",
       "      <td>MGN</td>\n",
       "      <td>https://arxiv.org/abs/1804.01438v3.pdf</td>\n",
       "      <td>Abstract:  The combination of global and parti...</td>\n",
       "      <td>Learning Discriminative Features with Multipl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>How far are we from solving the 2D &amp; 3D Face A...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.07332v3.pdf</td>\n",
       "      <td>Head Pose Estimation</td>\n",
       "      <td>AFLW2000</td>\n",
       "      <td>MAE</td>\n",
       "      <td>9.116</td>\n",
       "      <td>FAN (12 points)</td>\n",
       "      <td>https://arxiv.org/abs/1703.07332v3.pdf</td>\n",
       "      <td>Abstract:  This paper investigates how far a v...</td>\n",
       "      <td>How far are we from solving the 2D &amp; 3D Face ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Recurrent Batch Normalization</td>\n",
       "      <td>https://arxiv.org/pdf/1603.09025v5.pdf</td>\n",
       "      <td>Sequential Image Classification</td>\n",
       "      <td>Sequential MNIST</td>\n",
       "      <td>Unpermuted Accuracy</td>\n",
       "      <td>99%</td>\n",
       "      <td>BN LSTM</td>\n",
       "      <td>https://arxiv.org/abs/1603.09025v5.pdf</td>\n",
       "      <td>Abstract:  We propose a reparameterization of ...</td>\n",
       "      <td>Published as a conference paper at ICLR 2017 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Neural Paraphrase Identification of Questions ...</td>\n",
       "      <td>https://arxiv.org/pdf/1704.04565v2.pdf</td>\n",
       "      <td>Paraphrase Identification</td>\n",
       "      <td>Quora Question Pairs</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>88.4</td>\n",
       "      <td>pt-DecAtt</td>\n",
       "      <td>https://arxiv.org/abs/1704.04565v2.pdf</td>\n",
       "      <td>Abstract:  We present a solution to the proble...</td>\n",
       "      <td>Neural Paraphrase Identification of Questions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Pushing the bounds of dropout</td>\n",
       "      <td>https://arxiv.org/pdf/1805.09208v2.pdf</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>Penn Treebank (Word Level)</td>\n",
       "      <td>Validation perplexity</td>\n",
       "      <td>57.1</td>\n",
       "      <td>2-layer skip-LSTM + dropout tuning</td>\n",
       "      <td>https://arxiv.org/abs/1805.09208v2.pdf</td>\n",
       "      <td>Abstract:  We show that dropout training is be...</td>\n",
       "      <td>Under review as a conference paper at ICLR 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Feedback Network for Image Super-Resolution</td>\n",
       "      <td>https://arxiv.org/pdf/1903.09814v1.pdf</td>\n",
       "      <td>Image Super-Resolution</td>\n",
       "      <td>BSD100 - 4x upscaling</td>\n",
       "      <td>PSNR</td>\n",
       "      <td>27.72</td>\n",
       "      <td>SRFBN</td>\n",
       "      <td>https://arxiv.org/abs/1903.09814v1.pdf</td>\n",
       "      <td>Abstract:  Recent advances in image super-reso...</td>\n",
       "      <td>Feedback Network for Image Super-Resolution Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Exploring the Limits of Language Modeling</td>\n",
       "      <td>https://arxiv.org/pdf/1602.02410v2.pdf</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>One Billion Word</td>\n",
       "      <td>PPL</td>\n",
       "      <td>30</td>\n",
       "      <td>LSTM-8192-1024 + CNN Input</td>\n",
       "      <td>https://arxiv.org/abs/1602.02410v2.pdf</td>\n",
       "      <td>Abstract:  In this work we explore recent adva...</td>\n",
       "      <td>Exploring the Limits of Language Modeling Exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>CityPersons: A Diverse Dataset for Pedestrian ...</td>\n",
       "      <td>https://arxiv.org/pdf/1702.05693v1.pdf</td>\n",
       "      <td>Pedestrian Detection</td>\n",
       "      <td>CityPersons</td>\n",
       "      <td>Reasonable MR^-2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>FRCNN+Seg</td>\n",
       "      <td>https://arxiv.org/abs/1702.05693v1.pdf</td>\n",
       "      <td>Abstract:  Convnets have enabled significant p...</td>\n",
       "      <td>CityPersons: A Diverse Dataset for Pedestrian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Snips Voice Platform: an embedded Spoken Langu...</td>\n",
       "      <td>https://arxiv.org/pdf/1805.10190v3.pdf</td>\n",
       "      <td>Speech Recognition</td>\n",
       "      <td>LibriSpeech test-other</td>\n",
       "      <td>Word Error Rate (WER)</td>\n",
       "      <td>16.5</td>\n",
       "      <td>Snips</td>\n",
       "      <td>https://arxiv.org/abs/1805.10190v3.pdf</td>\n",
       "      <td>Abstract:  This paper presents the machine lea...</td>\n",
       "      <td>Snips Voice Platform: an embedded Spoken Lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Simple Recurrent Units for Highly Parallelizab...</td>\n",
       "      <td>https://arxiv.org/pdf/1709.02755v5.pdf</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>SQuAD1.1</td>\n",
       "      <td>EM</td>\n",
       "      <td>71.4</td>\n",
       "      <td>SRU</td>\n",
       "      <td>https://arxiv.org/abs/1709.02755v5.pdf</td>\n",
       "      <td>Abstract:  Common recurrent neural architectur...</td>\n",
       "      <td>Simple Recurrent Units for Highly Paralleliza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Semi-Supervised Learning with Ladder Networks</td>\n",
       "      <td>https://arxiv.org/pdf/1507.02672v2.pdf</td>\n",
       "      <td>Semi-Supervised Image Classification</td>\n",
       "      <td>CIFAR-10, 4000 Labels</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>79.6</td>\n",
       "      <td>Ì_åÒ-model</td>\n",
       "      <td>https://arxiv.org/abs/1507.02672v2.pdf</td>\n",
       "      <td>Abstract:  We combine supervised learning with...</td>\n",
       "      <td>Semi-Supervised Learning with Ladder Networks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Weighted Transformer Network for Machine Trans...</td>\n",
       "      <td>https://arxiv.org/pdf/1711.02132v1.pdf</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>WMT2014 English-French</td>\n",
       "      <td>BLEU score</td>\n",
       "      <td>41.4</td>\n",
       "      <td>Weighted Transformer (large)</td>\n",
       "      <td>https://arxiv.org/abs/1711.02132v1.pdf</td>\n",
       "      <td>Abstract:  State-of-the-art results on neural ...</td>\n",
       "      <td>WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>DCN+: Mixed Objective and Deep Residual Coatte...</td>\n",
       "      <td>https://arxiv.org/pdf/1711.00106v2.pdf</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>SQuAD1.1</td>\n",
       "      <td>EM</td>\n",
       "      <td>78.852</td>\n",
       "      <td>DCN+ (ensemble)</td>\n",
       "      <td>https://arxiv.org/abs/1711.00106v2.pdf</td>\n",
       "      <td>Abstract:  Traditional models for question ans...</td>\n",
       "      <td>DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Improving Neural Language Models with a Contin...</td>\n",
       "      <td>https://arxiv.org/pdf/1612.04426v1.pdf</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>WikiText-103</td>\n",
       "      <td>Test perplexity</td>\n",
       "      <td>48.7</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>https://arxiv.org/abs/1612.04426v1.pdf</td>\n",
       "      <td>Abstract:  We propose an extension to neural n...</td>\n",
       "      <td>Under review as a conference paper at ICLR 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>ENet: A Deep Neural Network Architecture for R...</td>\n",
       "      <td>https://arxiv.org/pdf/1606.02147v1.pdf</td>\n",
       "      <td>Real-Time Semantic Segmentation</td>\n",
       "      <td>Cityscapes</td>\n",
       "      <td>mIoU</td>\n",
       "      <td>58.30%</td>\n",
       "      <td>ENet</td>\n",
       "      <td>https://arxiv.org/abs/1606.02147v1.pdf</td>\n",
       "      <td>Abstract:  The ability to perform pixel-wise s...</td>\n",
       "      <td>ENet: A Deep Neural Network Architecture for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Revisiting Distributional Correspondence Index...</td>\n",
       "      <td>https://arxiv.org/pdf/1810.09311v1.pdf</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Multi-Domain Sentiment Dataset</td>\n",
       "      <td>DVD</td>\n",
       "      <td>81</td>\n",
       "      <td>Distributional Correspondence Indexing</td>\n",
       "      <td>https://arxiv.org/abs/1810.09311v1.pdf</td>\n",
       "      <td>Abstract:  This paper introduces PyDCI, a new ...</td>\n",
       "      <td>REVISITING DISTRIBUTIONAL CORRESPONDENCE INDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Learning to Cluster for Proposal-Free Instance...</td>\n",
       "      <td>https://arxiv.org/pdf/1803.06459v1.pdf</td>\n",
       "      <td>Lane Detection</td>\n",
       "      <td>TuSimple</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>96.50%</td>\n",
       "      <td>Pairwise pixel supervision + FCN</td>\n",
       "      <td>https://arxiv.org/abs/1803.06459v1.pdf</td>\n",
       "      <td>Abstract:  This work proposed a novel learning...</td>\n",
       "      <td>Learning to Cluster for Proposal-Free Instanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Shorten Spatial-spectral RNN with Parallel-GRU...</td>\n",
       "      <td>https://arxiv.org/pdf/1810.12563v1.pdf</td>\n",
       "      <td>Hyperspectral Image Classification</td>\n",
       "      <td>Indian Pines</td>\n",
       "      <td>Overall Accuracy</td>\n",
       "      <td>90.35%</td>\n",
       "      <td>St-SS-pGRU</td>\n",
       "      <td>https://arxiv.org/abs/1810.12563v1.pdf</td>\n",
       "      <td>Abstract:  Convolutional neural networks (CNNs...</td>\n",
       "      <td>Shorten Spatial-spectral RNN with Parallel-GR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Edinburgh Neural Machine Translation Systems f...</td>\n",
       "      <td>https://arxiv.org/pdf/1606.02891v2.pdf</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>WMT2016 Czech-English</td>\n",
       "      <td>BLEU score</td>\n",
       "      <td>31.4</td>\n",
       "      <td>Attentional encoder-decoder + BPE</td>\n",
       "      <td>https://arxiv.org/abs/1606.02891v2.pdf</td>\n",
       "      <td>Abstract:  We participated in the WMT 2016 sha...</td>\n",
       "      <td>() ar X iv :1 60 6. 02 89 1v 2 [ cs .C L ] 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Universal Sentence Encoder</td>\n",
       "      <td>https://arxiv.org/pdf/1803.11175v2.pdf</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>CR</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>87.45</td>\n",
       "      <td>USE_T+CNN (w2v w.e.)</td>\n",
       "      <td>https://arxiv.org/abs/1803.11175v2.pdf</td>\n",
       "      <td>Abstract:  We present models for encoding sent...</td>\n",
       "      <td>Universal Sentence Encoder Daniel Cera, Yinfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Semi-Amortized Variational Autoencoders</td>\n",
       "      <td>https://arxiv.org/pdf/1802.02550v7.pdf</td>\n",
       "      <td>Text Generation</td>\n",
       "      <td>Yahoo Questions</td>\n",
       "      <td>NLL</td>\n",
       "      <td>327.5</td>\n",
       "      <td>SA-VAE</td>\n",
       "      <td>https://arxiv.org/abs/1802.02550v7.pdf</td>\n",
       "      <td>Abstract:  Amortized variational inference (AV...</td>\n",
       "      <td>Semi-Amortized Variational Autoencoders Semi-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Weakly Supervised Deep Detection Networks</td>\n",
       "      <td>https://arxiv.org/pdf/1511.02853v4.pdf</td>\n",
       "      <td>Weakly Supervised Object Detection</td>\n",
       "      <td>PASCAL VOC 2007</td>\n",
       "      <td>MAP</td>\n",
       "      <td>39.3</td>\n",
       "      <td>WSDDN-Ens</td>\n",
       "      <td>https://arxiv.org/abs/1511.02853v4.pdf</td>\n",
       "      <td>Abstract:  Weakly supervised learning of objec...</td>\n",
       "      <td>Weakly Supervised Deep Detection Networks Hak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ReNet: A Recurrent Neural Network Based Altern...</td>\n",
       "      <td>https://arxiv.org/pdf/1505.00393v3.pdf</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>CIFAR-10</td>\n",
       "      <td>Percentage correct</td>\n",
       "      <td>87.7</td>\n",
       "      <td>ReNet</td>\n",
       "      <td>https://arxiv.org/abs/1505.00393v3.pdf</td>\n",
       "      <td>Abstract:  In this paper, we propose a deep ne...</td>\n",
       "      <td>ReNet: A Recurrent Neural Network Based Alter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Unsupervised Data Augmentation</td>\n",
       "      <td>https://arxiv.org/pdf/1904.12848.pdf</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Amazon Review Full</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>65.83</td>\n",
       "      <td>BERT large</td>\n",
       "      <td>https://arxiv.org/abs/1904.12848.pdf</td>\n",
       "      <td>Abstract:  Despite its success, deep learning ...</td>\n",
       "      <td>Unsupervised Data Augmentation Qizhe Xie1,2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Region Ensemble Network: Improving Convolution...</td>\n",
       "      <td>https://arxiv.org/pdf/1702.02447v2.pdf</td>\n",
       "      <td>Hand Pose Estimation</td>\n",
       "      <td>ICVL Hands</td>\n",
       "      <td>Average 3D Error</td>\n",
       "      <td>7.5</td>\n",
       "      <td>REN</td>\n",
       "      <td>https://arxiv.org/abs/1702.02447v2.pdf</td>\n",
       "      <td>Abstract:  Hand pose estimation from monocular...</td>\n",
       "      <td>arXiv:1702.02447v2 [cs.CV] 9 May 2017 ar X iv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Unsupervised Neural Machine Translation Initia...</td>\n",
       "      <td>https://arxiv.org/pdf/1810.12703v1.pdf</td>\n",
       "      <td>Unsupervised Machine Translation</td>\n",
       "      <td>WMT2016 English-German</td>\n",
       "      <td>BLEU</td>\n",
       "      <td>20</td>\n",
       "      <td>Synthetic bilingual data init</td>\n",
       "      <td>https://arxiv.org/abs/1810.12703v1.pdf</td>\n",
       "      <td>Abstract:  Recent work achieved remarkable res...</td>\n",
       "      <td>Unsupervised Neural Machine Translation Initi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Explicit Interaction Model towards Text Classi...</td>\n",
       "      <td>https://arxiv.org/pdf/1811.09386v1.pdf</td>\n",
       "      <td>Text Classification</td>\n",
       "      <td>AG News</td>\n",
       "      <td>Error</td>\n",
       "      <td>7</td>\n",
       "      <td>EXAM</td>\n",
       "      <td>https://arxiv.org/abs/1811.09386v1.pdf</td>\n",
       "      <td>Abstract:  Text classification is one of the f...</td>\n",
       "      <td>Explicit Interaction Model towards Text Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Pay Less Attention with Lightweight and Dynami...</td>\n",
       "      <td>https://arxiv.org/pdf/1901.10430v2.pdf</td>\n",
       "      <td>Language Modelling</td>\n",
       "      <td>One Billion Word</td>\n",
       "      <td>PPL</td>\n",
       "      <td>26.67</td>\n",
       "      <td>DynamicConv</td>\n",
       "      <td>https://arxiv.org/abs/1901.10430v2.pdf</td>\n",
       "      <td>Abstract:  Self-attention is a useful mechanis...</td>\n",
       "      <td>PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Single-Shot Refinement Neural Network for Obje...</td>\n",
       "      <td>https://arxiv.org/pdf/1711.06897v3.pdf</td>\n",
       "      <td>Object Detection</td>\n",
       "      <td>COCO</td>\n",
       "      <td>Bounding Box AP</td>\n",
       "      <td>41.8</td>\n",
       "      <td>RefineDet512+</td>\n",
       "      <td>https://arxiv.org/abs/1711.06897v3.pdf</td>\n",
       "      <td>Abstract:  For object detection, the two-stage...</td>\n",
       "      <td>Single-Shot Refinement Neural Network for Obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>LayoutNet: Reconstructing the 3D Room Layout f...</td>\n",
       "      <td>https://arxiv.org/pdf/1803.08999v1.pdf</td>\n",
       "      <td>3D Room Layouts From A Single Rgb Panorama</td>\n",
       "      <td>PanoContext</td>\n",
       "      <td>3DIoU</td>\n",
       "      <td>74.48%</td>\n",
       "      <td>LayoutNet</td>\n",
       "      <td>https://arxiv.org/abs/1803.08999v1.pdf</td>\n",
       "      <td>Abstract:  We propose an algorithm to predict ...</td>\n",
       "      <td>LayoutNet: Reconstructing the 3D Room Layout ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Bridging Saliency Detection to Weakly Supervis...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.01290v1.pdf</td>\n",
       "      <td>Weakly Supervised Object Detection</td>\n",
       "      <td>PASCAL VOC 2007</td>\n",
       "      <td>MAP</td>\n",
       "      <td>31.3</td>\n",
       "      <td>Self-paced curriculum learning</td>\n",
       "      <td>https://arxiv.org/abs/1703.01290v1.pdf</td>\n",
       "      <td>Abstract:  Weakly-supervised object detection ...</td>\n",
       "      <td>Bridging Saliency Detection to Weakly Supervi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>StarMap for Category-Agnostic Keypoint and Vie...</td>\n",
       "      <td>https://arxiv.org/pdf/1803.09331v2.pdf</td>\n",
       "      <td>Keypoint Detection</td>\n",
       "      <td>Pascal3D+</td>\n",
       "      <td>Mean PCK</td>\n",
       "      <td>78.6</td>\n",
       "      <td>StarMap</td>\n",
       "      <td>https://arxiv.org/abs/1803.09331v2.pdf</td>\n",
       "      <td>Abstract:  Semantic keypoints provide concise ...</td>\n",
       "      <td>StarMap for Category-Agnostic Keypoint and Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Rotational Unit of Memory</td>\n",
       "      <td>https://arxiv.org/pdf/1710.09537v1.pdf</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>bAbi</td>\n",
       "      <td>Accuracy (trained on 1k)</td>\n",
       "      <td>73.20%</td>\n",
       "      <td>RUM</td>\n",
       "      <td>https://arxiv.org/abs/1710.09537v1.pdf</td>\n",
       "      <td>Abstract:  The concepts of unitary evolution m...</td>\n",
       "      <td>ROTATIONAL UNIT OF MEMORY Rumen Dangovski∗ Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Rethinking Atrous Convolution for Semantic Ima...</td>\n",
       "      <td>https://arxiv.org/pdf/1706.05587v3.pdf</td>\n",
       "      <td>Semantic Segmentation</td>\n",
       "      <td>PASCAL VOC 2012</td>\n",
       "      <td>Mean IoU</td>\n",
       "      <td>86.90%</td>\n",
       "      <td>DeepLabv3-JFT</td>\n",
       "      <td>https://arxiv.org/abs/1706.05587v3.pdf</td>\n",
       "      <td>Abstract:  In this work, we revisit atrous con...</td>\n",
       "      <td>Rethinking Atrous Convolution for Semantic Im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Towards End-to-End Lane Detection: an Instance...</td>\n",
       "      <td>https://arxiv.org/pdf/1802.05591v1.pdf</td>\n",
       "      <td>Lane Detection</td>\n",
       "      <td>TuSimple</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>96.40%</td>\n",
       "      <td>LaneNet</td>\n",
       "      <td>https://arxiv.org/abs/1802.05591v1.pdf</td>\n",
       "      <td>Abstract:  Modern cars are incorporating an in...</td>\n",
       "      <td>Towards End-to-End Lane Detection: an Instanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SyncSpecCNN: Synchronized Spectral CNN for 3D ...</td>\n",
       "      <td>https://arxiv.org/pdf/1612.00606v1.pdf</td>\n",
       "      <td>3D Part Segmentation</td>\n",
       "      <td>ShapeNet-Part</td>\n",
       "      <td>Class Average IoU</td>\n",
       "      <td>82</td>\n",
       "      <td>SSCNN</td>\n",
       "      <td>https://arxiv.org/abs/1612.00606v1.pdf</td>\n",
       "      <td>Abstract:  In this paper, we study the problem...</td>\n",
       "      <td>SyncSpecCNN: Synchronized Spectral CNN for 3D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ContextLocNet: Context-Aware Deep Network Mode...</td>\n",
       "      <td>https://arxiv.org/pdf/1609.04331v1.pdf</td>\n",
       "      <td>Weakly Supervised Object Detection</td>\n",
       "      <td>PASCAL VOC 2007</td>\n",
       "      <td>MAP</td>\n",
       "      <td>36.3</td>\n",
       "      <td>WSDDN + context</td>\n",
       "      <td>https://arxiv.org/abs/1609.04331v1.pdf</td>\n",
       "      <td>Abstract:  We aim to localize objects in image...</td>\n",
       "      <td>ContextLocNet: Context-Aware Deep Network Mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paper_title  \\\n",
       "0   The LovÌÁsz-Softmax loss: A tractable surrogat...   \n",
       "1   Bottom-Up and Top-Down Attention for Image Cap...   \n",
       "2                                 Deep Fried Convnets   \n",
       "3   Learning Natural Language Inference using Bidi...   \n",
       "4   Cascade R-CNN: Delving into High Quality Objec...   \n",
       "5   Fast and Accurate Deep Network Learning by Exp...   \n",
       "6   Language Modeling with Gated Convolutional Net...   \n",
       "7   Pervasive Attention: 2D Convolutional Neural N...   \n",
       "8   Deep Bayesian Bandits Showdown: An Empirical C...   \n",
       "9   CoupleNet: Coupling Global Structure with Loca...   \n",
       "10  Semi-Supervised Classification with Graph Conv...   \n",
       "11  The Best of Both Worlds: Combining Recent Adva...   \n",
       "12  Brain Tumor Segmentation with Deep Neural Netw...   \n",
       "13  Conditional Image Synthesis With Auxiliary Cla...   \n",
       "14  Supervised Learning of Universal Sentence Repr...   \n",
       "15  Adversarial Training Methods for Semi-Supervis...   \n",
       "16  On the Role of Text Preprocessing in Neural Ne...   \n",
       "17  Natural TTS Synthesis by Conditioning WaveNet ...   \n",
       "18  Scale-Aware Trident Networks for Object Detection   \n",
       "19  Deeply learned face representations are sparse...   \n",
       "20  Hyperspectral Image Classification with Markov...   \n",
       "21  A Deep Relevance Matching Model for Ad-hoc Ret...   \n",
       "22  Towards a Better Match in Siamese Network Base...   \n",
       "23  Learning Discriminative Features with Multiple...   \n",
       "24  How far are we from solving the 2D & 3D Face A...   \n",
       "25                      Recurrent Batch Normalization   \n",
       "26  Neural Paraphrase Identification of Questions ...   \n",
       "27                      Pushing the bounds of dropout   \n",
       "28        Feedback Network for Image Super-Resolution   \n",
       "29          Exploring the Limits of Language Modeling   \n",
       "..                                                ...   \n",
       "70  CityPersons: A Diverse Dataset for Pedestrian ...   \n",
       "71  Snips Voice Platform: an embedded Spoken Langu...   \n",
       "72  Simple Recurrent Units for Highly Parallelizab...   \n",
       "73      Semi-Supervised Learning with Ladder Networks   \n",
       "74  Weighted Transformer Network for Machine Trans...   \n",
       "75  DCN+: Mixed Objective and Deep Residual Coatte...   \n",
       "76  Improving Neural Language Models with a Contin...   \n",
       "77  ENet: A Deep Neural Network Architecture for R...   \n",
       "78  Revisiting Distributional Correspondence Index...   \n",
       "79  Learning to Cluster for Proposal-Free Instance...   \n",
       "80  Shorten Spatial-spectral RNN with Parallel-GRU...   \n",
       "81  Edinburgh Neural Machine Translation Systems f...   \n",
       "82                         Universal Sentence Encoder   \n",
       "83            Semi-Amortized Variational Autoencoders   \n",
       "84          Weakly Supervised Deep Detection Networks   \n",
       "85  ReNet: A Recurrent Neural Network Based Altern...   \n",
       "86                     Unsupervised Data Augmentation   \n",
       "87  Region Ensemble Network: Improving Convolution...   \n",
       "88  Unsupervised Neural Machine Translation Initia...   \n",
       "89  Explicit Interaction Model towards Text Classi...   \n",
       "90  Pay Less Attention with Lightweight and Dynami...   \n",
       "91  Single-Shot Refinement Neural Network for Obje...   \n",
       "92  LayoutNet: Reconstructing the 3D Room Layout f...   \n",
       "93  Bridging Saliency Detection to Weakly Supervis...   \n",
       "94  StarMap for Category-Agnostic Keypoint and Vie...   \n",
       "95                          Rotational Unit of Memory   \n",
       "96  Rethinking Atrous Convolution for Semantic Ima...   \n",
       "97  Towards End-to-End Lane Detection: an Instance...   \n",
       "98  SyncSpecCNN: Synchronized Spectral CNN for 3D ...   \n",
       "99  ContextLocNet: Context-Aware Deep Network Mode...   \n",
       "\n",
       "                                 paper_url  \\\n",
       "0   https://arxiv.org/pdf/1705.08790v2.pdf   \n",
       "1   https://arxiv.org/pdf/1707.07998v3.pdf   \n",
       "2    https://arxiv.org/pdf/1412.7149v4.pdf   \n",
       "3   https://arxiv.org/pdf/1605.09090v1.pdf   \n",
       "4   https://arxiv.org/pdf/1712.00726v1.pdf   \n",
       "5   https://arxiv.org/pdf/1511.07289v5.pdf   \n",
       "6   https://arxiv.org/pdf/1612.08083v3.pdf   \n",
       "7   https://arxiv.org/pdf/1808.03867v3.pdf   \n",
       "8   https://arxiv.org/pdf/1802.09127v1.pdf   \n",
       "9   https://arxiv.org/pdf/1708.02863v1.pdf   \n",
       "10  https://arxiv.org/pdf/1609.02907v4.pdf   \n",
       "11  https://arxiv.org/pdf/1804.09849v2.pdf   \n",
       "12  https://arxiv.org/pdf/1505.03540v3.pdf   \n",
       "13  https://arxiv.org/pdf/1610.09585v4.pdf   \n",
       "14  https://arxiv.org/pdf/1705.02364v5.pdf   \n",
       "15  https://arxiv.org/pdf/1605.07725v3.pdf   \n",
       "16  https://arxiv.org/pdf/1707.01780v3.pdf   \n",
       "17  https://arxiv.org/pdf/1712.05884v2.pdf   \n",
       "18  https://arxiv.org/pdf/1901.01892v1.pdf   \n",
       "19   https://arxiv.org/pdf/1412.1265v1.pdf   \n",
       "20  https://arxiv.org/pdf/1705.00727v2.pdf   \n",
       "21  https://arxiv.org/pdf/1711.08611v1.pdf   \n",
       "22  https://arxiv.org/pdf/1809.01368v1.pdf   \n",
       "23  https://arxiv.org/pdf/1804.01438v3.pdf   \n",
       "24  https://arxiv.org/pdf/1703.07332v3.pdf   \n",
       "25  https://arxiv.org/pdf/1603.09025v5.pdf   \n",
       "26  https://arxiv.org/pdf/1704.04565v2.pdf   \n",
       "27  https://arxiv.org/pdf/1805.09208v2.pdf   \n",
       "28  https://arxiv.org/pdf/1903.09814v1.pdf   \n",
       "29  https://arxiv.org/pdf/1602.02410v2.pdf   \n",
       "..                                     ...   \n",
       "70  https://arxiv.org/pdf/1702.05693v1.pdf   \n",
       "71  https://arxiv.org/pdf/1805.10190v3.pdf   \n",
       "72  https://arxiv.org/pdf/1709.02755v5.pdf   \n",
       "73  https://arxiv.org/pdf/1507.02672v2.pdf   \n",
       "74  https://arxiv.org/pdf/1711.02132v1.pdf   \n",
       "75  https://arxiv.org/pdf/1711.00106v2.pdf   \n",
       "76  https://arxiv.org/pdf/1612.04426v1.pdf   \n",
       "77  https://arxiv.org/pdf/1606.02147v1.pdf   \n",
       "78  https://arxiv.org/pdf/1810.09311v1.pdf   \n",
       "79  https://arxiv.org/pdf/1803.06459v1.pdf   \n",
       "80  https://arxiv.org/pdf/1810.12563v1.pdf   \n",
       "81  https://arxiv.org/pdf/1606.02891v2.pdf   \n",
       "82  https://arxiv.org/pdf/1803.11175v2.pdf   \n",
       "83  https://arxiv.org/pdf/1802.02550v7.pdf   \n",
       "84  https://arxiv.org/pdf/1511.02853v4.pdf   \n",
       "85  https://arxiv.org/pdf/1505.00393v3.pdf   \n",
       "86    https://arxiv.org/pdf/1904.12848.pdf   \n",
       "87  https://arxiv.org/pdf/1702.02447v2.pdf   \n",
       "88  https://arxiv.org/pdf/1810.12703v1.pdf   \n",
       "89  https://arxiv.org/pdf/1811.09386v1.pdf   \n",
       "90  https://arxiv.org/pdf/1901.10430v2.pdf   \n",
       "91  https://arxiv.org/pdf/1711.06897v3.pdf   \n",
       "92  https://arxiv.org/pdf/1803.08999v1.pdf   \n",
       "93  https://arxiv.org/pdf/1703.01290v1.pdf   \n",
       "94  https://arxiv.org/pdf/1803.09331v2.pdf   \n",
       "95  https://arxiv.org/pdf/1710.09537v1.pdf   \n",
       "96  https://arxiv.org/pdf/1706.05587v3.pdf   \n",
       "97  https://arxiv.org/pdf/1802.05591v1.pdf   \n",
       "98  https://arxiv.org/pdf/1612.00606v1.pdf   \n",
       "99  https://arxiv.org/pdf/1609.04331v1.pdf   \n",
       "\n",
       "                                          task  \\\n",
       "0                        Semantic Segmentation   \n",
       "1                    Visual Question Answering   \n",
       "2                         Image Classification   \n",
       "3                   Natural Language Inference   \n",
       "4                             Object Detection   \n",
       "5                         Image Classification   \n",
       "6                           Language Modelling   \n",
       "7                          Machine Translation   \n",
       "8                          Multi-Armed Bandits   \n",
       "9                             Object Detection   \n",
       "10                         Node Classification   \n",
       "11                         Machine Translation   \n",
       "12                    Brain Tumor Segmentation   \n",
       "13                Conditional Image Generation   \n",
       "14                 Semantic Textual Similarity   \n",
       "15                          Sentiment Analysis   \n",
       "16                          Sentiment Analysis   \n",
       "17                            Speech Synthesis   \n",
       "18                            Object Detection   \n",
       "19                           Face Verification   \n",
       "20          Hyperspectral Image Classification   \n",
       "21                Ad-Hoc Information Retrieval   \n",
       "22                      Visual Object Tracking   \n",
       "23                    Person Re-Identification   \n",
       "24                        Head Pose Estimation   \n",
       "25             Sequential Image Classification   \n",
       "26                   Paraphrase Identification   \n",
       "27                          Language Modelling   \n",
       "28                      Image Super-Resolution   \n",
       "29                          Language Modelling   \n",
       "..                                         ...   \n",
       "70                        Pedestrian Detection   \n",
       "71                          Speech Recognition   \n",
       "72                          Question Answering   \n",
       "73        Semi-Supervised Image Classification   \n",
       "74                         Machine Translation   \n",
       "75                          Question Answering   \n",
       "76                          Language Modelling   \n",
       "77             Real-Time Semantic Segmentation   \n",
       "78                          Sentiment Analysis   \n",
       "79                              Lane Detection   \n",
       "80          Hyperspectral Image Classification   \n",
       "81                         Machine Translation   \n",
       "82                          Sentiment Analysis   \n",
       "83                             Text Generation   \n",
       "84          Weakly Supervised Object Detection   \n",
       "85                        Image Classification   \n",
       "86                          Sentiment Analysis   \n",
       "87                        Hand Pose Estimation   \n",
       "88            Unsupervised Machine Translation   \n",
       "89                         Text Classification   \n",
       "90                          Language Modelling   \n",
       "91                            Object Detection   \n",
       "92  3D Room Layouts From A Single Rgb Panorama   \n",
       "93          Weakly Supervised Object Detection   \n",
       "94                          Keypoint Detection   \n",
       "95                          Question Answering   \n",
       "96                       Semantic Segmentation   \n",
       "97                              Lane Detection   \n",
       "98                        3D Part Segmentation   \n",
       "99          Weakly Supervised Object Detection   \n",
       "\n",
       "                                              dataset  \\\n",
       "0                                     PASCAL VOC 2012   \n",
       "1   COCO Visual Question Answering (VQA) real imag...   \n",
       "2                                               MNIST   \n",
       "3                                                SNLI   \n",
       "4                                                COCO   \n",
       "5                                            CIFAR-10   \n",
       "6                                    One Billion Word   \n",
       "7                            IWSLT2015 English-German   \n",
       "8                                            Mushroom   \n",
       "9                                     PASCAL VOC 2007   \n",
       "10                                           Citeseer   \n",
       "11                             WMT2014 English-French   \n",
       "12                                         BRATS-2013   \n",
       "13                                           CIFAR-10   \n",
       "14                                           SentEval   \n",
       "15                                               IMDb   \n",
       "16                                               IMDb   \n",
       "17                             North American English   \n",
       "18                                               COCO   \n",
       "19                          Labeled Faces in the Wild   \n",
       "20                                       Indian Pines   \n",
       "21                                      TREC Robust04   \n",
       "22                                         VOT2017/18   \n",
       "23                                      DukeMTMC-reID   \n",
       "24                                           AFLW2000   \n",
       "25                                   Sequential MNIST   \n",
       "26                               Quora Question Pairs   \n",
       "27                         Penn Treebank (Word Level)   \n",
       "28                              BSD100 - 4x upscaling   \n",
       "29                                   One Billion Word   \n",
       "..                                                ...   \n",
       "70                                        CityPersons   \n",
       "71                             LibriSpeech test-other   \n",
       "72                                           SQuAD1.1   \n",
       "73                              CIFAR-10, 4000 Labels   \n",
       "74                             WMT2014 English-French   \n",
       "75                                           SQuAD1.1   \n",
       "76                                       WikiText-103   \n",
       "77                                         Cityscapes   \n",
       "78                     Multi-Domain Sentiment Dataset   \n",
       "79                                           TuSimple   \n",
       "80                                       Indian Pines   \n",
       "81                              WMT2016 Czech-English   \n",
       "82                                                 CR   \n",
       "83                                    Yahoo Questions   \n",
       "84                                    PASCAL VOC 2007   \n",
       "85                                           CIFAR-10   \n",
       "86                                 Amazon Review Full   \n",
       "87                                         ICVL Hands   \n",
       "88                             WMT2016 English-German   \n",
       "89                                            AG News   \n",
       "90                                   One Billion Word   \n",
       "91                                               COCO   \n",
       "92                                        PanoContext   \n",
       "93                                    PASCAL VOC 2007   \n",
       "94                                          Pascal3D+   \n",
       "95                                               bAbi   \n",
       "96                                    PASCAL VOC 2012   \n",
       "97                                           TuSimple   \n",
       "98                                      ShapeNet-Part   \n",
       "99                                    PASCAL VOC 2007   \n",
       "\n",
       "                       metric_name metric_value  \\\n",
       "0                         Mean IoU       79.00%   \n",
       "1               Percentage correct        70.34   \n",
       "2                 Percentage error          0.7   \n",
       "3                  % Test Accuracy         83.3   \n",
       "4                  Bounding Box AP         42.8   \n",
       "5               Percentage correct         93.5   \n",
       "6                              PPL         31.9   \n",
       "7                       BLEU score        27.99   \n",
       "8                Cumulative regret         1.92   \n",
       "9                              MAP       82.70%   \n",
       "10                        Accuracy       70.30%   \n",
       "11                      BLEU score        41.0*   \n",
       "12                      Dice Score         0.88   \n",
       "13                 Inception score         8.25   \n",
       "14                            MRPC    76.2/83.1   \n",
       "15                        Accuracy         94.1   \n",
       "16                        Accuracy         88.9   \n",
       "17              Mean Opinion Score        4.526   \n",
       "18                 Bounding Box AP         48.4   \n",
       "19                        Accuracy       99.47%   \n",
       "20                Overall Accuracy       96.12%   \n",
       "21                             MAP        0.279   \n",
       "22  Expected Average Overlap (EAO)        0.337   \n",
       "23                          Rank-1         88.7   \n",
       "24                             MAE        9.116   \n",
       "25             Unpermuted Accuracy          99%   \n",
       "26                        Accuracy         88.4   \n",
       "27           Validation perplexity         57.1   \n",
       "28                            PSNR        27.72   \n",
       "29                             PPL           30   \n",
       "..                             ...          ...   \n",
       "70                Reasonable MR^-2         14.8   \n",
       "71           Word Error Rate (WER)         16.5   \n",
       "72                              EM         71.4   \n",
       "73                        Accuracy         79.6   \n",
       "74                      BLEU score         41.4   \n",
       "75                              EM       78.852   \n",
       "76                 Test perplexity         48.7   \n",
       "77                            mIoU       58.30%   \n",
       "78                             DVD           81   \n",
       "79                        Accuracy       96.50%   \n",
       "80                Overall Accuracy       90.35%   \n",
       "81                      BLEU score         31.4   \n",
       "82                        Accuracy        87.45   \n",
       "83                             NLL        327.5   \n",
       "84                             MAP         39.3   \n",
       "85              Percentage correct         87.7   \n",
       "86                        Accuracy        65.83   \n",
       "87                Average 3D Error          7.5   \n",
       "88                            BLEU           20   \n",
       "89                           Error            7   \n",
       "90                             PPL        26.67   \n",
       "91                 Bounding Box AP         41.8   \n",
       "92                           3DIoU       74.48%   \n",
       "93                             MAP         31.3   \n",
       "94                        Mean PCK         78.6   \n",
       "95        Accuracy (trained on 1k)       73.20%   \n",
       "96                        Mean IoU       86.90%   \n",
       "97                        Accuracy       96.40%   \n",
       "98               Class Average IoU           82   \n",
       "99                             MAP         36.3   \n",
       "\n",
       "                                     model  \\\n",
       "0      Deeplab-v2 with Lovasz-Softmax loss   \n",
       "1                                  Up-Down   \n",
       "2                      Deep Fried Convnets   \n",
       "3           600D (300+300) BiLSTM encoders   \n",
       "4                            Cascade R-CNN   \n",
       "5                 Exponential Linear Units   \n",
       "6                       GCNN-14 bottleneck   \n",
       "7                      Pervasive Attention   \n",
       "8            NeuralLinear FullPosterior-MR   \n",
       "9                                CoupleNet   \n",
       "10                                     GCN   \n",
       "11                                   RNMT+   \n",
       "12                         InputCascadeCNN   \n",
       "13                                  AC-GAN   \n",
       "14                               InferSent   \n",
       "15            Virtual adversarial training   \n",
       "16                                CNN+LSTM   \n",
       "17                              Tacotron 2   \n",
       "18                              TridentNet   \n",
       "19                                DeepId2+   \n",
       "20                                 CNN-MRF   \n",
       "21                                    DRMM   \n",
       "22                               SA Siam R   \n",
       "23                                     MGN   \n",
       "24                         FAN (12 points)   \n",
       "25                                 BN LSTM   \n",
       "26                               pt-DecAtt   \n",
       "27      2-layer skip-LSTM + dropout tuning   \n",
       "28                                   SRFBN   \n",
       "29              LSTM-8192-1024 + CNN Input   \n",
       "..                                     ...   \n",
       "70                               FRCNN+Seg   \n",
       "71                                   Snips   \n",
       "72                                     SRU   \n",
       "73                              Ì_åÒ-model   \n",
       "74            Weighted Transformer (large)   \n",
       "75                         DCN+ (ensemble)   \n",
       "76                                    LSTM   \n",
       "77                                    ENet   \n",
       "78  Distributional Correspondence Indexing   \n",
       "79        Pairwise pixel supervision + FCN   \n",
       "80                              St-SS-pGRU   \n",
       "81       Attentional encoder-decoder + BPE   \n",
       "82                    USE_T+CNN (w2v w.e.)   \n",
       "83                                  SA-VAE   \n",
       "84                               WSDDN-Ens   \n",
       "85                                   ReNet   \n",
       "86                              BERT large   \n",
       "87                                     REN   \n",
       "88           Synthetic bilingual data init   \n",
       "89                                    EXAM   \n",
       "90                             DynamicConv   \n",
       "91                           RefineDet512+   \n",
       "92                               LayoutNet   \n",
       "93          Self-paced curriculum learning   \n",
       "94                                 StarMap   \n",
       "95                                     RUM   \n",
       "96                           DeepLabv3-JFT   \n",
       "97                                 LaneNet   \n",
       "98                                   SSCNN   \n",
       "99                         WSDDN + context   \n",
       "\n",
       "                                 paper_abs  \\\n",
       "0   https://arxiv.org/abs/1705.08790v2.pdf   \n",
       "1   https://arxiv.org/abs/1707.07998v3.pdf   \n",
       "2    https://arxiv.org/abs/1412.7149v4.pdf   \n",
       "3   https://arxiv.org/abs/1605.09090v1.pdf   \n",
       "4   https://arxiv.org/abs/1712.00726v1.pdf   \n",
       "5   https://arxiv.org/abs/1511.07289v5.pdf   \n",
       "6   https://arxiv.org/abs/1612.08083v3.pdf   \n",
       "7   https://arxiv.org/abs/1808.03867v3.pdf   \n",
       "8   https://arxiv.org/abs/1802.09127v1.pdf   \n",
       "9   https://arxiv.org/abs/1708.02863v1.pdf   \n",
       "10  https://arxiv.org/abs/1609.02907v4.pdf   \n",
       "11  https://arxiv.org/abs/1804.09849v2.pdf   \n",
       "12  https://arxiv.org/abs/1505.03540v3.pdf   \n",
       "13  https://arxiv.org/abs/1610.09585v4.pdf   \n",
       "14  https://arxiv.org/abs/1705.02364v5.pdf   \n",
       "15  https://arxiv.org/abs/1605.07725v3.pdf   \n",
       "16  https://arxiv.org/abs/1707.01780v3.pdf   \n",
       "17  https://arxiv.org/abs/1712.05884v2.pdf   \n",
       "18  https://arxiv.org/abs/1901.01892v1.pdf   \n",
       "19   https://arxiv.org/abs/1412.1265v1.pdf   \n",
       "20  https://arxiv.org/abs/1705.00727v2.pdf   \n",
       "21  https://arxiv.org/abs/1711.08611v1.pdf   \n",
       "22  https://arxiv.org/abs/1809.01368v1.pdf   \n",
       "23  https://arxiv.org/abs/1804.01438v3.pdf   \n",
       "24  https://arxiv.org/abs/1703.07332v3.pdf   \n",
       "25  https://arxiv.org/abs/1603.09025v5.pdf   \n",
       "26  https://arxiv.org/abs/1704.04565v2.pdf   \n",
       "27  https://arxiv.org/abs/1805.09208v2.pdf   \n",
       "28  https://arxiv.org/abs/1903.09814v1.pdf   \n",
       "29  https://arxiv.org/abs/1602.02410v2.pdf   \n",
       "..                                     ...   \n",
       "70  https://arxiv.org/abs/1702.05693v1.pdf   \n",
       "71  https://arxiv.org/abs/1805.10190v3.pdf   \n",
       "72  https://arxiv.org/abs/1709.02755v5.pdf   \n",
       "73  https://arxiv.org/abs/1507.02672v2.pdf   \n",
       "74  https://arxiv.org/abs/1711.02132v1.pdf   \n",
       "75  https://arxiv.org/abs/1711.00106v2.pdf   \n",
       "76  https://arxiv.org/abs/1612.04426v1.pdf   \n",
       "77  https://arxiv.org/abs/1606.02147v1.pdf   \n",
       "78  https://arxiv.org/abs/1810.09311v1.pdf   \n",
       "79  https://arxiv.org/abs/1803.06459v1.pdf   \n",
       "80  https://arxiv.org/abs/1810.12563v1.pdf   \n",
       "81  https://arxiv.org/abs/1606.02891v2.pdf   \n",
       "82  https://arxiv.org/abs/1803.11175v2.pdf   \n",
       "83  https://arxiv.org/abs/1802.02550v7.pdf   \n",
       "84  https://arxiv.org/abs/1511.02853v4.pdf   \n",
       "85  https://arxiv.org/abs/1505.00393v3.pdf   \n",
       "86    https://arxiv.org/abs/1904.12848.pdf   \n",
       "87  https://arxiv.org/abs/1702.02447v2.pdf   \n",
       "88  https://arxiv.org/abs/1810.12703v1.pdf   \n",
       "89  https://arxiv.org/abs/1811.09386v1.pdf   \n",
       "90  https://arxiv.org/abs/1901.10430v2.pdf   \n",
       "91  https://arxiv.org/abs/1711.06897v3.pdf   \n",
       "92  https://arxiv.org/abs/1803.08999v1.pdf   \n",
       "93  https://arxiv.org/abs/1703.01290v1.pdf   \n",
       "94  https://arxiv.org/abs/1803.09331v2.pdf   \n",
       "95  https://arxiv.org/abs/1710.09537v1.pdf   \n",
       "96  https://arxiv.org/abs/1706.05587v3.pdf   \n",
       "97  https://arxiv.org/abs/1802.05591v1.pdf   \n",
       "98  https://arxiv.org/abs/1612.00606v1.pdf   \n",
       "99  https://arxiv.org/abs/1609.04331v1.pdf   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Abstract:  The Jaccard index, also referred to...   \n",
       "1   Abstract:  Top-down visual attention mechanism...   \n",
       "2   Abstract:  The fully connected layers of a dee...   \n",
       "3   Abstract:  In this paper, we proposed a senten...   \n",
       "4   Abstract:  In object detection, an intersectio...   \n",
       "5   Abstract:  We introduce the \"exponential linea...   \n",
       "6   Abstract:  The pre-dominant approach to langua...   \n",
       "7   Abstract:  Current state-of-the-art machine tr...   \n",
       "8   Abstract:  Recent advances in deep reinforceme...   \n",
       "9   Abstract:  The region-based Convolutional Neur...   \n",
       "10  Abstract:  We present a scalable approach for ...   \n",
       "11  Abstract:  The past year has witnessed rapid a...   \n",
       "12  Abstract:  In this paper, we present a fully a...   \n",
       "13  Abstract:  Synthesizing high resolution photor...   \n",
       "14  Abstract:  Many modern NLP systems rely on wor...   \n",
       "15  Abstract:  Adversarial training provides a mea...   \n",
       "16  Abstract:  Text preprocessing is often the fir...   \n",
       "17  Abstract:  This paper describes Tacotron 2, a ...   \n",
       "18  Abstract:  Scale variation is one of the key c...   \n",
       "19  Abstract:  This paper designs a high-performan...   \n",
       "20  Abstract:  This paper presents a new supervise...   \n",
       "21  Abstract:  In recent years, deep neural networ...   \n",
       "22  Abstract:  Recently, Siamese network based tra...   \n",
       "23  Abstract:  The combination of global and parti...   \n",
       "24  Abstract:  This paper investigates how far a v...   \n",
       "25  Abstract:  We propose a reparameterization of ...   \n",
       "26  Abstract:  We present a solution to the proble...   \n",
       "27  Abstract:  We show that dropout training is be...   \n",
       "28  Abstract:  Recent advances in image super-reso...   \n",
       "29  Abstract:  In this work we explore recent adva...   \n",
       "..                                                ...   \n",
       "70  Abstract:  Convnets have enabled significant p...   \n",
       "71  Abstract:  This paper presents the machine lea...   \n",
       "72  Abstract:  Common recurrent neural architectur...   \n",
       "73  Abstract:  We combine supervised learning with...   \n",
       "74  Abstract:  State-of-the-art results on neural ...   \n",
       "75  Abstract:  Traditional models for question ans...   \n",
       "76  Abstract:  We propose an extension to neural n...   \n",
       "77  Abstract:  The ability to perform pixel-wise s...   \n",
       "78  Abstract:  This paper introduces PyDCI, a new ...   \n",
       "79  Abstract:  This work proposed a novel learning...   \n",
       "80  Abstract:  Convolutional neural networks (CNNs...   \n",
       "81  Abstract:  We participated in the WMT 2016 sha...   \n",
       "82  Abstract:  We present models for encoding sent...   \n",
       "83  Abstract:  Amortized variational inference (AV...   \n",
       "84  Abstract:  Weakly supervised learning of objec...   \n",
       "85  Abstract:  In this paper, we propose a deep ne...   \n",
       "86  Abstract:  Despite its success, deep learning ...   \n",
       "87  Abstract:  Hand pose estimation from monocular...   \n",
       "88  Abstract:  Recent work achieved remarkable res...   \n",
       "89  Abstract:  Text classification is one of the f...   \n",
       "90  Abstract:  Self-attention is a useful mechanis...   \n",
       "91  Abstract:  For object detection, the two-stage...   \n",
       "92  Abstract:  We propose an algorithm to predict ...   \n",
       "93  Abstract:  Weakly-supervised object detection ...   \n",
       "94  Abstract:  Semantic keypoints provide concise ...   \n",
       "95  Abstract:  The concepts of unitary evolution m...   \n",
       "96  Abstract:  In this work, we revisit atrous con...   \n",
       "97  Abstract:  Modern cars are incorporating an in...   \n",
       "98  Abstract:  In this paper, we study the problem...   \n",
       "99  Abstract:  We aim to localize objects in image...   \n",
       "\n",
       "                                           paper_text  \n",
       "0    Accepted as a conference paper at CVPR 2018 T...  \n",
       "1    Bottom-Up and Top-Down Attention for Image Ca...  \n",
       "2    Deep Fried Convnets Zichao Yang1 Marcin Moczu...  \n",
       "3    () ar X iv :1 60 5. 09 09 0v 1 [ cs .C L ] 3 ...  \n",
       "4    ar X iv :1 71 2. 00 72 6v 1 [ cs .C V ] 3 D e...  \n",
       "5    Published as a conference paper at ICLR 2016 ...  \n",
       "6    Language Modeling with Gated Convolutional Ne...  \n",
       "7    Pervasive Attention: 2D Convolutional Neural ...  \n",
       "8    Published as a conference paper at ICLR 2018 ...  \n",
       "9    CoupleNet: Coupling Global Structure with Loc...  \n",
       "10   Published as a conference paper at ICLR 2017 ...  \n",
       "11   The Best of Both Worlds: Combining Recent Adv...  \n",
       "12   Brain Tumor Segmentation with Deep Neural Net...  \n",
       "13   Conditional Image Synthesis with Auxiliary Cl...  \n",
       "14   ar X iv :1 70 5. 02 36 4v 5 [ cs .C L ] 8 J u...  \n",
       "15   () ar X iv :1 60 5. 07 72 5v 3 [ st at .M L ]...  \n",
       "16   ar X iv :1 70 7. 01 78 0v 3 [ cs .C L ] 2 3 A...  \n",
       "17   NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET...  \n",
       "18   Scale-Aware Trident Networks for Object Detec...  \n",
       "19   Deeply learned face representations are spars...  \n",
       "20   1 Hyperspectral Image Classification with Mar...  \n",
       "21   ar X iv :1 71 1. 08 61 1v 1 [ cs .I R ] 2 3 N...  \n",
       "22   Towards a Better Match in Siamese Network Bas...  \n",
       "23   Learning Discriminative Features with Multipl...  \n",
       "24   How far are we from solving the 2D & 3D Face ...  \n",
       "25   Published as a conference paper at ICLR 2017 ...  \n",
       "26   Neural Paraphrase Identification of Questions...  \n",
       "27   Under review as a conference paper at ICLR 20...  \n",
       "28   Feedback Network for Image Super-Resolution Z...  \n",
       "29   Exploring the Limits of Language Modeling Exp...  \n",
       "..                                                ...  \n",
       "70   CityPersons: A Diverse Dataset for Pedestrian...  \n",
       "71   Snips Voice Platform: an embedded Spoken Lang...  \n",
       "72   Simple Recurrent Units for Highly Paralleliza...  \n",
       "73   Semi-Supervised Learning with Ladder Networks...  \n",
       "74   WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRAN...  \n",
       "75   DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATT...  \n",
       "76   Under review as a conference paper at ICLR 20...  \n",
       "77   ENet: A Deep Neural Network Architecture for ...  \n",
       "78   REVISITING DISTRIBUTIONAL CORRESPONDENCE INDE...  \n",
       "79   Learning to Cluster for Proposal-Free Instanc...  \n",
       "80   Shorten Spatial-spectral RNN with Parallel-GR...  \n",
       "81   () ar X iv :1 60 6. 02 89 1v 2 [ cs .C L ] 2 ...  \n",
       "82   Universal Sentence Encoder Daniel Cera, Yinfe...  \n",
       "83   Semi-Amortized Variational Autoencoders Semi-...  \n",
       "84   Weakly Supervised Deep Detection Networks Hak...  \n",
       "85   ReNet: A Recurrent Neural Network Based Alter...  \n",
       "86   Unsupervised Data Augmentation Qizhe Xie1,2, ...  \n",
       "87   arXiv:1702.02447v2 [cs.CV] 9 May 2017 ar X iv...  \n",
       "88   Unsupervised Neural Machine Translation Initi...  \n",
       "89   Explicit Interaction Model towards Text Class...  \n",
       "90   PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAM...  \n",
       "91   Single-Shot Refinement Neural Network for Obj...  \n",
       "92   LayoutNet: Reconstructing the 3D Room Layout ...  \n",
       "93   Bridging Saliency Detection to Weakly Supervi...  \n",
       "94   StarMap for Category-Agnostic Keypoint and Vi...  \n",
       "95   ROTATIONAL UNIT OF MEMORY Rumen Dangovski∗ Ma...  \n",
       "96   Rethinking Atrous Convolution for Semantic Im...  \n",
       "97   Towards End-to-End Lane Detection: an Instanc...  \n",
       "98   SyncSpecCNN: Synchronized Spectral CNN for 3D...  \n",
       "99   ContextLocNet: Context-Aware Deep Network Mod...  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_4 = df_3[1:41]\n",
    "\n",
    "# df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "momentum\n",
    "\n",
    "epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-2-model",
   "language": "python",
   "name": "gpt-2-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

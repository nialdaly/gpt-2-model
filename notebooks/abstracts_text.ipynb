{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>introduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>Reading text in natural images is a challengin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>Recurrent neural networks (RNNs) have been suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pythia v0.1: the Winning Entry to the VQA Chal...</td>\n",
       "      <td>We present Pythia v0.1, a modular framework fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1                           Neural Semantic Encoders   \n",
       "2  Pythia v0.1: the Winning Entry to the VQA Chal...   \n",
       "\n",
       "                                        introduction  \n",
       "0  Reading text in natural images is a challengin...  \n",
       "1  Recurrent neural networks (RNNs) have been suc...  \n",
       "2  We present Pythia v0.1, a modular framework fo...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_excel('../data/paper_metadata.xlsx', index_col=False)\n",
    "metadata = metadata[['paper_title', 'introduction']]\n",
    "metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>The model described here is called SegLink. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>The model described here is called 300D NSE en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pythia v0.1: the Winning Entry to the VQA Chal...</td>\n",
       "      <td>The model described here is called Pythia v0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1                           Neural Semantic Encoders   \n",
       "2  Pythia v0.1: the Winning Entry to the VQA Chal...   \n",
       "\n",
       "                                                 abs  \n",
       "0  The model described here is called SegLink. Th...  \n",
       "1  The model described here is called 300D NSE en...  \n",
       "2  The model described here is called Pythia v0.1...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df = pd.read_csv('../data/metadata_abstracts.csv')\n",
    "abstracts_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataframe joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>abs</th>\n",
       "      <th>introduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>The model described here is called SegLink. Th...</td>\n",
       "      <td>Reading text in natural images is a challengin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>The model described here is called 300D NSE en...</td>\n",
       "      <td>Recurrent neural networks (RNNs) have been suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pythia v0.1: the Winning Entry to the VQA Chal...</td>\n",
       "      <td>The model described here is called Pythia v0.1...</td>\n",
       "      <td>We present Pythia v0.1, a modular framework fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large Scale GAN Training for High Fidelity Nat...</td>\n",
       "      <td>The model described here is called BigGAN. The...</td>\n",
       "      <td>The state of generative image modeling has adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Margin-based Parallel Corpus Mining with Multi...</td>\n",
       "      <td>The model described here is called Europarl. T...</td>\n",
       "      <td>Given the prevalence of corpus-based approache...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1                           Neural Semantic Encoders   \n",
       "2  Pythia v0.1: the Winning Entry to the VQA Chal...   \n",
       "3  Large Scale GAN Training for High Fidelity Nat...   \n",
       "4  Margin-based Parallel Corpus Mining with Multi...   \n",
       "\n",
       "                                                 abs  \\\n",
       "0  The model described here is called SegLink. Th...   \n",
       "1  The model described here is called 300D NSE en...   \n",
       "2  The model described here is called Pythia v0.1...   \n",
       "3  The model described here is called BigGAN. The...   \n",
       "4  The model described here is called Europarl. T...   \n",
       "\n",
       "                                        introduction  \n",
       "0  Reading text in natural images is a challengin...  \n",
       "1  Recurrent neural networks (RNNs) have been suc...  \n",
       "2  We present Pythia v0.1, a modular framework fo...  \n",
       "3  The state of generative image modeling has adv...  \n",
       "4  Given the prevalence of corpus-based approache...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df = pd.merge(abstracts_df, metadata, on='paper_title')\n",
    "abstracts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, index in abstracts_df.iterrows():\n",
    "    \n",
    "#     print(abstracts_df['abs'].iloc[i])\n",
    "    \n",
    "#     print(\"\\n\")\n",
    "    \n",
    "#     print(abstracts_df['introduction'].iloc[i])\n",
    "    \n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_intro = []\n",
    "\n",
    "for index, row in abstracts_df.iterrows():    \n",
    "    abstract_intro.append(row['abs'] + \"\\n\\n\" + row['introduction'] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The model described here is called SegLink. The metric used to validate the model was F-measure with the model being tested on the ICDAR 2015 Incidental Text dataset. A F-measure of 0.75 was achieved. The learning rate was set to 0.001. A batch size of 32.0 was used. A momentum value of 0.9 was used. The TensorFlow framework was used to develop the model. \\n\\nReading text in natural images is a challenging task under active research. It is driven by many real-world applications, such as Photo OCR [2], geo-location, and image retrieval [9]. In a text reading system, text detection, i.e. localizing text with bounding boxes of words or text lines, is usually the first step of great significance. In a sense, text detection can be seen as object detection applied to text, where words/characters/text lines are taken as the detection targets. Owing to this, a new trend has emerged recently that state-of-the-art text detection methods [9, 6, 22, 30] are heavily based on the advanced general object detection or segmentation techniques, e.g. [4, 5, 15]. Despite the great success of the previous work, we argue that the general detection methods are not well suited for text detection, for two main reasons. First, word/text line bounding boxes have much larger aspect ratios than those of general objects. An (fast/faster) R-CNN [5, 4, 19]- or SSD [14]-style detector may suffer from the difficulty of producing such boxes, owing to its proposal or anchor box design. In addition, some non-Latin text does not have blank spaces between words, hence the even larger bounding box aspect ratios, which make the problem worse. Second, unlike general objects, text usually has a clear definition of orientation [25]. It is important for a text detector to produce oriented boxes. However, most general object detection methods are not designed to produce oriented boxes. To overcome the above challenges, we tackle the text detection problem in a new perspective. We propose to decompose long text into two smaller and locally-detectable elements, namely segment and link. As illustrated in Fig. 1, a segment is an oriented box that covers a part of a word (for clarity we use “word” here and later on, but segments also work seamlessly on text lines that comprise multiple words); A link connects a pair of adjacent segments, indicating that they belong to the same word. Under the above definitions, a word is located by a number of segments with links between them. During detection, segments and links are densely detected on an input image by a convolutional neural network. Then, the segments are combined into whole words according to the links. The key advantage of this approach is that long and oriented text is now detected locally since both basic elements are locally-detectable: Detecting a segment does not require the whole word to be observed. And neither does a link since the connection of two segments can be inferred from a local context. Thereafter, we can detect text of any length and orientation with great flexibility and efficiency. Concretely, we propose a convolutional neural network (CNN) model to detect both segments and links simultaneously, in a fully-convolutional manner. The network uses VGG-16 [21] as its backbone. A few extra feature layers are added onto it. Convolutional predictors are added to 6 of the feature layers to detect segments and links at different scales. To deal with redundant detections, we introduce two types of links, namely within-layer links and cross-layer links. A within-layer link connects a segment to its neighbors on the same layer. A cross-layer link, on the other hand, connects a segment to its neighbors on the lower layer. In this way, we connect segments of adjacent locations as well as scales. Finally, we find connected segments with a depth-first search (DFS) algorithm and combine them into whole words. Our main contribution is the novel segment-linking detection method. Through experiments, we show that the proposed method possesses several distinctive advantages over the other state-of-the-art methods: 1) Robustness: SegLink models the structure of oriented text in a simple and elegant way, with robustness against complex backgrounds. Our method achieves highly competitive results on standard datasets. In particular, it outperforms the previous best by a large margin in terms of f-measure (75.0% vs 64.8%) on the ICDAR 2015 Incidental (Challenge 4) benchmark [12]; 2) Efficiency: SegLink is highly efficient due to its single-pass, fully-convolutional design. It processes more than 20 images of 512x512 size per second; 3) Generality: Without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese. We demonstrate this capability on a multi-lingual dataset.\\n\\n']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_intro[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export list as a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/abs_data.txt', 'w') as myfile:  \n",
    "    myfile.write('\\n\\n'.join(str(line) for line in abstract_intro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-2-model",
   "language": "python",
   "name": "gpt-2-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

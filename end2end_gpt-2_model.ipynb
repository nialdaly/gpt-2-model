{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nialdaly/Documents/gpt-2_model/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loading checkpoint checkpoint/run1/model-3000\n",
      "WARNING:tensorflow:From /Users/nialdaly/Documents/gpt-2_model/venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-3000\n"
     ]
    }
   ],
   "source": [
    "sess_1 = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess_1, run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess_1 = gpt2.start_tf_sess()\n",
    "# gpt2.load_gpt2(sess_1, run_name='run1')\n",
    "\n",
    "def intro_output(formalised_abs):\n",
    "    \n",
    "    gpt_2_intro = gpt2.generate(sess_1,\n",
    "        length = 500,\n",
    "        temperature = 0.7,\n",
    "#         prefix = formalised_abs,\n",
    "        nsamples = 5,\n",
    "        batch_size = 5,\n",
    "        return_as_list=True\n",
    "        )[0]\n",
    "\n",
    "    return gpt_2_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model described here is called HRNet-48. The metric used to validate the model was AP with the model being tested on the COCO dataset. A AP of 75.1 was achieved. The learning rate was set to 0.001. A batch size of 256.0 was used. The model was trained with a total of 100.0\n"
     ]
    }
   ],
   "source": [
    "formalised_abs = \"The model described here is called HRNet-48. The metric used to validate the model was AP with the model being tested on the COCO dataset. A AP of 75.1 was achieved. The learning rate was set to 0.001. A batch size of 256.0 was used. The model was trained with a total of 100.0\"\n",
    "print(formalised_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nialdaly/Documents/gpt-2_model/venv/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:71: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/nialdaly/Documents/gpt-2_model/venv/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n"
     ]
    }
   ],
   "source": [
    "model_1_res = intro_output(formalised_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIDER MEDALS: A COMPLEX EXPERIPERSON study that builds on previous work (Moree et al., 2016; Vaswani et al., 2018). We begin with a review of related work in deep learning, end-to-end speech recognition, and scalability in Section 5. Section 6 concludes with a description of our experimental results and analysis of end-to-end optimisation.\n",
      "\n",
      "\n",
      "\n",
      "The model described here is called DGI. The metric used to validate the model was Accuracy with the model being tested on the Citeseer dataset. A Accuracy of 0.718 was achieved. The learning rate was set to 0.001. The model was trained with a total of 20.0 epochs. \n",
      "\n",
      "Generalizing neural networks to graph-structured inputs is one of the current major challenges of machine learning (Bronstein et al., 2017; Hamilton et al., 2017a; Battaglia et al., 2018). While significant strides have recently been made, notably with graph convolutional networks (Kipf & Welling, 2016a; Gilmer et al., 2017; Velickovi ˇ c et al., 2018), most successful methods use ´ supervised learning, which is often not possible as most graph data in the wild is unlabeled. In addition, it is often desirable to discover novel or interesting structure from large-scale graphs, and as such, unsupervised graph learning is essential for many important tasks. Currently, the dominant algorithms for unsupervised representation learning with graph-structured data rely on random walk-based objectives (Grover & Leskovec, 2016; Perozzi et al., 2014; Tang et al., 2015; Hamilton et al., 2017a), sometimes further simplified to reconstruct adjacency information (Kipf & Welling, 2016b; Duran & Niepert, 2017). The underlying intuition is to train an encoder network so that nodes that are “close” in the input graph are also “close” in the representation space. While powerful—and related to traditional metrics such as the personalized PageRank score (Jeh & Widom, 2003)—random walk methods suffer from known limitations. Most prominently, the random-walk objective is known to over-emphasize proximity information at the expense of structural information (Ribeiro et al., 2017), and performance on large-scale graphs is notoriously difficult. Moreover, with\n"
     ]
    }
   ],
   "source": [
    "print(model_1_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run2/model-2000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run2/model-2000\n"
     ]
    }
   ],
   "source": [
    "sess_2 = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess_2, run_name='run2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess_2 = gpt2.start_tf_sess()\n",
    "# gpt2.load_gpt2(sess_2, run_name='run2')\n",
    "\n",
    "def abs_output(model_1_res):\n",
    "    \n",
    "    gpt_2_abs = gpt2.generate(sess_2,\n",
    "        length = 500,\n",
    "        temperature = 0.7,\n",
    "#         prefix = model_1_res,\n",
    "        nsamples = 5,\n",
    "        batch_size = 5,\n",
    "        return_as_list=True\n",
    "        )[0]\n",
    "\n",
    "    return gpt_2_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_3_res = \"Scaling up ConvNets is widely used to achieve better accuracy. For example, ResNet (He et al., 2016) can be scaled up from ResNet-18 to ResNet-200 by using more layers; Recently, GPipe (Huang et al., 2018) achieved 84.3% ImageNet top-1 accuracy by scaling up a baseline model four time larger. However, the process of scaling up ConvNets has never been well understood and there are currently many ways to do it. The most common way is to scale up ConvNets by their depth (He et al., 2016) or width (Zagoruyko & Komodakis, 2016). Another less common, but increasingly popular, method is to scale up models by image resolution (Huang et al., 2018). In previous work, it is common to scale only one of the three dimensions – depth, width, and image size. Though it is possible to scale two or three dimensions arbitrarily, arbitrary scaling requires tedious manual tuning and still often yields sub-optimal accuracy and efficiency. In this paper, we want to study and rethink the process of scaling up ConvNets. In particular, we investigate the central question: is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency? Our empirical study shows that it is critical to balance all dimensions of network width/depth/resolution, and surprisingly such balance can be achieved by simply scaling each of them with constant ratio. Based on this observation, we propose a simple yet effective compound scaling method. Unlike conventional practice that arbitrary scales these factors, our method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use 2 N times more computational resources, then we can simply increase the network depth by α N , width by β N , and image size by γ N , where α, β, γ are constant coefficients determined by a small grid search on the original small model. Figure 2 illustrates the difference between our scaling method and conventional methods. Intuitively, the compound scaling method makes sense because if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. In fact, previous theoretical (Raghu et al., 2017; Lu et al., 2018) and empirical results (Zagoruyko & Komodakis, 2016) both show that there exists certain relationship between network width and depth, but to our best knowledge, we are the first to empirically quantify the relationship among all three dimensions of network width, depth, and resolution. We demonstrate that our scaling method work well on existing MobileNets (Howard et al., 2017; Sandler et al., 2018) and ResNet (He et al., 2016). Notably, the effectiveness of model scaling heavily depends on the baseline network; to go even further, we use neural architecture search (Zoph & Le, 2017; Tan et al., 2019) to develop a new baseline network, and scale it up to obtain a family of models, called EfficientNets. Figure 1 summarizes the ImageNet performance, where our EfficientNets significantly outperform other ConvNets. In particular, our EfficientNet-B7 surpasses the best existing GPipe accuracy (Huang et al., 2018), but using 8.4x fewer parameters and running 6.1x faster on inference. Compared to the widely used ResNet-50 (He et al., 2016), our EfficientNet-B4 improves the top-1 accuracy from 76.3% to 82.6% (+6.3%) with similar FLOPS. Besides ImageNet, EfficientNets also transfer well and achieve stateof-the-art accuracy on 5 out of 8 widely used datasets, while reducing parameters by up to 21x than existing ConvNets.\"\n",
    "# print(model_3_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2_res = abs_output(model_3_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this work, we study a new paradigm of consuming neural networks with neural network training data, namely, an ever-more-powerful neural network model that can be trained agnostically on ever-larger amounts of labeled training data. To this end, we introduce the ever-more-powerful neural network model (EMNet), which has never been trained end-to-end. This model can train richer for neural networks that are far more susceptible to memory corruption, as shown in Figure 1 (c). In addition, we use the EMNet with ResNet blocks (ResNet) to view the relations between layers as a sequence of close and nelements (close ones) and as a sequence of close and neares (near ones)). To explore this transformation using neural networks, we train the same neural network model three times a day for three days, from which information is extracted by modifying certain convolutional layers that are used to train more powerful models. We show that this training procedure does not only improves the model accuracy but also the performance, showing that by improving the accuracy of the neural network model we can improve the performance of a neural network model trained on a single image. We empirically both demonstrate the effectiveness of the EMNet and under-perform (below) the reproducibility of models trained on a single dataset.\n",
      "\n",
      "We introduce the ever-more powerful neural network model (EMNet) with the ever-more powerful memory, by reforming the sequence-to-sequence model (Figure 1), we can train much more powerful models that have been trained on massive amounts of labeled data. We introduce the ever-more powerful neural network model (EMNet), which has the same accuracy as a neural network without any neural network layer (except for the right-most one) or layer-wise activations. We introduce the ever-more powerful neural network model (EMNet), which has the same performance as a neural network without any layer-wise activations. We show that without any layer-wise activations we can outperform models trained on a single image on many datasets. We also demonstrate that we can improve the performance of multiple powerful neural network models by adding an additional layer of network instability in the convolutional layer. This instability prevents the same number of hidden units from interacting with the same information in each layer. We empirically both demonstrate the effectiveness of the EMNet and under-perform (below) the reproducibility of models trained\n"
     ]
    }
   ],
   "source": [
    "model_2_res = abs_output(model_3_res)\n",
    "print(model_2_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1905.11946v2.pdf\n",
    "# https://github.com/kratzert/finetune_alexnet_with_tensorflow/issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-2-model",
   "language": "python",
   "name": "gpt-2-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

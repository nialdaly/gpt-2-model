{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = gpt2.start_tf_sess()\n",
    "# sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2.load_gpt2(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nialdaly/Documents/gpt-2_model/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loading checkpoint checkpoint/run1/model-3000\n",
      "WARNING:tensorflow:From /Users/nialdaly/Documents/gpt-2_model/venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-3000\n"
     ]
    }
   ],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output(start_sentence):\n",
    "    # start_sentence = \"Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method.\"\n",
    "    #\n",
    "    single_text = gpt2.generate(sess,\n",
    "        length = 500,\n",
    "        temperature = 0.7,\n",
    "        prefix = start_sentence,\n",
    "        nsamples = 5,\n",
    "        batch_size = 5,\n",
    "        return_as_list=True\n",
    "        )[0]\n",
    "\n",
    "    return single_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# start_sentence = \"Self-driving cars are one of the most promising prospects for near term Artificial Intelligence research.\"\n",
    "\n",
    "start_sentence = \"The model described here is called Pythia v0.1. The metric used to validate the model was Accuracy with the model being tested on the VQA v2.0 dataset. A Accuracy of 0.7024 was achieved. The learning rate was set to 0.002. A batch size of 512.0 was used.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/nialdaly/Documents/gpt-2_model/venv/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:71: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/nialdaly/Documents/gpt-2_model/venv/lib/python3.6/site-packages/gpt_2_simple/src/sample.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "result = model_output(start_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T model described here is called Pythia v0.1. The metric used to validate the model was Accuracy with the model being tested on the VQA v2.0 dataset. A Accuracy of 0.7024 was achieved. The learning rate was set to 0.002. A batch size of 512.0 was used. \n",
      "\n",
      "We present Pythia v0.1, a modular framework for Visual Question Answering research, which formed the basis for the winning entry to the VQA Challenge 2018 from Facebook AI Research (FAIR)’s A-STAR2 team. The motivation for Pythia comes from the following observation – a majority of today’s Visual Question Answering (VQA) models fit a particular design paradigm, with modules for question encoding, image feature extraction, fusion of the two (typically with attention), and classification over the space of answers. The long-term goal of Pythia is to serve as a platform for easy and modular research & development in VQA [2] and related directions like visual dialog [3]. The name ‘Pythia’ is an homage to the Oracle of Apollo at Delphi, who answered questions in Ancient Greece. The starting point for Pythia v0.1 is a modular reimplementation of the bottom-up top-down (up-down) model [14]. In this study, we demonstrate that by making a sequence of subtle but important changes, we can significantly improve the performance as summarized in Table 13 .\n",
      "\n",
      "\n",
      "\n",
      "The model described here is called BigGAN. The metric used to validate the model was FID with the model being tested on the ImageNet 128x128  dataset. A FID of 7.4 was achieved. The learning rate was set to 0.0002. A batch size of 2048.0 was used. The TensorFlow framework was used to develop the model. \n",
      "\n",
      "The state of generative image modeling has advanced dramatically in recent years, with Generative Adversarial Networks (GANs, Goodfellow et al. (2014)) at the forefront of efforts to generate highfidelity, diverse images with models learned directly from data. GAN training is dynamic, and sensitive to nearly every aspect of its setup (from optimization parameters to model architecture), but a torrent of research has yielded empirical and theoretical insights enabling stable training in a variety of settings. Despite this progress, the current state of the art in conditional ImageNet modeling (Zhang et al., 2018) achieves an Inception Score (Salimans et al., 2016) of 52.5, compared to 233 for real data. In this work, we set out to close the gap in fidelity and variety between images generated by GANs and real-world images from the\n"
     ]
    }
   ],
   "source": [
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sentence2 = \"The model described here is called Pythia v0.1. The metric used to validate the model was Accuracy with the model being tested on the VQA v2.0 dataset. A Accuracy of 0.9 was achieved. The learning rate was set to 0.01.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T model described here is called Pythia v0.1. The metric used to validate the model was Accuracy with the model being tested on the VQA v2.0 dataset. A Accuracy of 0.9 was achieved. The learning rate was set to 0.01. A batch size of 32.0 was used. \n",
      "\n",
      "We present Pythia v0.1, a modular framework for Visual Question Answering research, which formed the basis for the winning entry to the VQA Challenge 2018 from Facebook AI Research (FAIR)’s A-STAR2 team. The motivation for Pythia comes from the following observation – a majority of today’s Visual Question Answering (VQA) models fit a particular design paradigm, with modules for question encoding, image feature extraction, fusion of the two (typically with attention), and classification over the space of answers. The long-term goal of Pythia is to serve as a platform for easy and modular research & development in VQA [2] and related directions like visual dialog [3]. The name ‘Pythia’ is an homage to the Oracle of Apollo at Delphi, who answered questions in Ancient Greece. The starting point for Pythia v0.1 is a modular reimplementation of the bottom-up top-down (up-down) model [14]. In this study, we demonstrate that by making a sequence of subtle but important changes, we can significantly improve the performance as summarized in Table 13 .\n",
      "\n",
      "\n",
      "\n",
      "The model described here is called BigGAN. The metric used to validate the model was FID with the model being tested on the ImageNet 128x128  dataset. A FID of 7.4 was achieved. The learning rate was set to 0.0002. A batch size of 2048.0 was used. The TensorFlow framework was used to develop the model. \n",
      "\n",
      "The state of generative image modeling has advanced dramatically in recent years, with Generative Adversarial Networks (GANs, Goodfellow et al. (2014)) at the forefront of efforts to generate highfidelity, diverse images with models learned directly from data. GAN training is dynamic, and sensitive to nearly every aspect of its setup (from optimization parameters to model architecture), but a torrent of research has yielded empirical and theoretical insights enabling stable training at high resolutions. GANs have quickly become the de facto standard in many computer vision applications, with models from both large and small datasets [6, 23, 29, 21, 43] reaching state of the art results. Yet, they are limited in both computational freedom and modeling capacity. GANs struggle to handle spatially variant noise, nor concatenate\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "result2 = model_output(start_sentence2)\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start_sentence = \"comma.ai’s approach to Artificial Intelligence for self-driving cars\"\n",
    "# start_sentence = \"comma.ai’s approach to Artificial Intelligence for self-driving cars\"\n",
    "\n",
    "# # single_text = gpt2.generate(sess, return_as_list=True)[0]\n",
    "# print(model_output(start_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-2-model",
   "language": "python",
   "name": "gpt-2-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
